"""
Word æ–‡æ¡£å¤„ç†æœåŠ¡

å®ç° Word æ–‡æ¡£çš„è§£æã€åˆ†å—å’Œ Episode åˆ›å»º
"""
from docx import Document
from typing import Dict, List, Any, Optional
from datetime import datetime
import os
import logging
import uuid
from app.core.graphiti_client import get_graphiti_instance
from app.models.graphiti_entities import ENTITY_TYPES, EDGE_TYPES, EDGE_TYPE_MAP

logger = logging.getLogger(__name__)


class WordDocumentService:
    """Word æ–‡æ¡£å¤„ç†æœåŠ¡"""
    
    @staticmethod
    def _parse_word_document(file_path: str, document_id: str = None) -> Dict[str, Any]:
        """
        è§£æ Word æ–‡æ¡£ï¼Œæå–æ–‡å­—ã€å›¾ç‰‡ã€é“¾æ¥ã€è¡¨æ ¼
        
        Returns:
            {
                "text_content": str,
                "structured_content": list,
                "images": list,
                "links": list,
                "tables": list,
                "metadata": dict
            }
        """
        doc = Document(file_path)
        
        result = {
            "text_content": "",
            "structured_content": [],
            "images": [],
            "links": [],
            "ole_objects": [],  # æ–°å¢ï¼šOLEå¯¹è±¡ï¼ˆåµŒå…¥æ–‡æ¡£ï¼‰
            "tables": [],
            "metadata": {
                "title": doc.core_properties.title or "",
                "author": doc.core_properties.author or "",
                "created": doc.core_properties.created,
                "modified": doc.core_properties.modified,
            }
        }
        
        # å¯¼å…¥å¿…è¦çš„ç±»
        from docx.oxml.text.paragraph import CT_P
        from docx.oxml.table import CT_Tbl
        from docx.table import Table
        from docx.text.paragraph import Paragraph
        
        # å…ˆæå–æ‰€æœ‰å›¾ç‰‡ï¼Œå»ºç«‹ä½ç½®æ˜ å°„
        result["images"] = WordDocumentService._extract_images_from_document(doc, document_id, file_path)
        image_position_map = {}  # è®°å½•å›¾ç‰‡åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®ï¼ˆæ®µè½ç´¢å¼• -> å›¾ç‰‡åˆ—è¡¨ï¼‰
        unmatched_images = []  # è®°å½•æœªåŒ¹é…ä½ç½®çš„å›¾ç‰‡
        
        # è¡¨æ ¼è®¡æ•°å™¨ï¼ˆæŒ‰æ–‡æ¡£é¡ºåºåˆ†é…table_idï¼‰
        table_counter = 0
        # OLEå¯¹è±¡è®¡æ•°å™¨ï¼ˆæŒ‰æ–‡æ¡£é¡ºåºåˆ†é…ole_idï¼‰
        ole_counter = 0
        
        for img in result["images"]:
            pos = img.get("position", -1)
            # å¦‚æœpositionæ˜¯-1ï¼Œè¯´æ˜å›¾ç‰‡æœªåŒ¹é…åˆ°ä½ç½®ï¼Œå…ˆè®°å½•åˆ°unmatched_images
            if pos == -1:
                unmatched_images.append(img)
                logger.warning(f"å›¾ç‰‡ {img.get('image_id', 'unknown')} æœªåŒ¹é…åˆ°æ®µè½ä½ç½® (rel_id: {img.get('rel_id', 'None')})")
            else:
                # position >= 0 çš„å›¾ç‰‡æ­£å¸¸æ˜ å°„
                if pos not in image_position_map:
                    image_position_map[pos] = []
                image_position_map[pos].append(img)
        
        # å¯¹äºæœªåŒ¹é…çš„å›¾ç‰‡ï¼Œå°è¯•é€šè¿‡é¡ºåºæ¨æ–­ä½ç½®ï¼ˆå¤‡ç”¨ç­–ç•¥ï¼‰
        if unmatched_images:
            logger.info(f"å‘ç° {len(unmatched_images)} å¼ æœªåŒ¹é…ä½ç½®çš„å›¾ç‰‡ï¼Œå°†ä½¿ç”¨å¤‡ç”¨ç­–ç•¥")
            # è¿™äº›å›¾ç‰‡å°†åœ¨åç»­å¤„ç†ä¸­ï¼Œæ ¹æ®å®ƒä»¬åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°é¡ºåºæ¥æ¨æ–­ä½ç½®
        
        # æ„å»ºç« èŠ‚æ ‡é¢˜æ˜ å°„ï¼ˆç”¨äºå›¾ç‰‡ä¸Šä¸‹æ–‡å’Œæè¿°ç”Ÿæˆï¼‰
        section_titles = []
        for para_idx_temp, para in enumerate(doc.paragraphs):
            if para.style.name.startswith('Heading'):
                section_titles.append((para_idx_temp, para.text.strip()))
        logger.debug(f"æ„å»ºç« èŠ‚æ ‡é¢˜æ˜ å°„: {len(section_titles)} ä¸ªç« èŠ‚")
        
        # è§£ææ–‡æ¡£å†…å®¹
        para_idx = 0  # æ®µè½ç´¢å¼•è®¡æ•°å™¨
        for element in doc.element.body:
            if isinstance(element, CT_P):
                # æ®µè½
                paragraph = Paragraph(element, doc)
                # æå–æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼ˆä¿ç•™åŠ ç²—ã€æ–œä½“ç­‰æ ¼å¼ï¼‰
                text = WordDocumentService._extract_formatted_text(paragraph)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜
                is_heading = paragraph.style.name.startswith('Heading')
                heading_level = 0
                if is_heading:
                    try:
                        heading_level = int(paragraph.style.name.split()[-1])
                    except:
                        heading_level = 1
                
                # æå–é“¾æ¥
                links = WordDocumentService._extract_links_from_paragraph(paragraph)
                if links:
                    result["links"].extend(links)
                
                # æå–OLEå¯¹è±¡ï¼ˆåµŒå…¥æ–‡æ¡£ï¼‰
                ole_objects = WordDocumentService._extract_ole_objects_from_paragraph(paragraph, para_idx, document_id, file_path)
                if ole_objects:
                    # ä¸ºæ¯ä¸ªOLEå¯¹è±¡åˆ†é…å›ºå®šçš„ole_idï¼ˆæŒ‰æ–‡æ¡£é¡ºåºï¼‰
                    for ole_obj in ole_objects:
                        ole_counter += 1
                        ole_id = f"ole_{ole_counter}"
                        ole_obj["ole_id"] = ole_id
                        
                        # å¦‚æœæœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œéœ€è¦é‡å‘½å
                        if ole_obj.get("temp_ole_id") and ole_obj.get("file_path") and ole_obj.get("file_ext"):
                            import os
                            temp_ole_id = ole_obj["temp_ole_id"]
                            old_file_path = ole_obj["file_path"]
                            file_ext = ole_obj["file_ext"]
                            
                            # æ„å»ºæ–°æ–‡ä»¶è·¯å¾„
                            new_file_path = old_file_path.replace(temp_ole_id, ole_id)
                            new_relative_path = ole_obj.get("relative_path", "").replace(temp_ole_id, ole_id)
                            
                            # é‡å‘½åæ–‡ä»¶
                            try:
                                if os.path.exists(old_file_path):
                                    os.rename(old_file_path, new_file_path)
                                    ole_obj["file_path"] = new_file_path
                                    ole_obj["relative_path"] = new_relative_path
                                    logger.debug(f"OLEå¯¹è±¡æ–‡ä»¶é‡å‘½å: {old_file_path} -> {new_file_path}")
                                    
                                    # æ›´æ–°æ˜¾ç¤ºåç§°ï¼Œä½¿ç”¨å®é™…ä¿å­˜çš„æ‰©å±•å
                                    if file_ext and file_ext != '.bin':
                                        current_name = ole_obj.get("name", "")
                                        # ç§»é™¤æ—§çš„æ‰©å±•åï¼ˆ.binæˆ–å…¶ä»–ï¼‰
                                        base_name = os.path.splitext(current_name)[0]
                                        if not base_name or base_name.endswith('.bin') or 'oleObject' in base_name:
                                            base_name = "åµŒå…¥æ–‡æ¡£"
                                        ole_obj["name"] = f"{base_name}{file_ext}"
                                        logger.info(f"æ›´æ–°OLEå¯¹è±¡æ˜¾ç¤ºåç§°: {ole_obj['name']} (æ ¼å¼: {file_ext})")
                            except Exception as e:
                                logger.warning(f"é‡å‘½åOLEå¯¹è±¡æ–‡ä»¶å¤±è´¥: {e}")
                            
                            # æ¸…ç†ä¸´æ—¶å­—æ®µ
                            ole_obj.pop("temp_ole_id", None)
                            ole_obj.pop("file_ext", None)
                    
                    result["ole_objects"].extend(ole_objects)
                
                # æ£€æŸ¥å½“å‰æ®µè½ä½ç½®æ˜¯å¦æœ‰å›¾ç‰‡
                images_in_para = image_position_map.get(para_idx, [])
                
                # å¯¹äºæœªåŒ¹é…çš„å›¾ç‰‡ï¼Œå¦‚æœå½“å‰æ®µè½åŒ…å«å›¾ç‰‡ç›¸å…³çš„å…³é”®è¯ï¼Œå°è¯•å…³è”
                # è¿™æ˜¯ä¸€ä¸ªå¤‡ç”¨ç­–ç•¥ï¼Œç”¨äºå¤„ç†æ— æ³•é€šè¿‡å…³ç³»IDåŒ¹é…çš„å›¾ç‰‡
                if unmatched_images and not images_in_para:
                    # æ£€æŸ¥æ®µè½æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡ç›¸å…³çš„å…³é”®è¯
                    para_text_lower = text.lower()
                    image_keywords = ['å›¾', 'æµç¨‹å›¾', 'ç¤ºæ„å›¾', 'å›¾ç‰‡', 'å›¾è¡¨', 'figure', 'image']
                    if any(keyword in para_text_lower for keyword in image_keywords):
                        # å¦‚æœæ®µè½åŒ…å«å›¾ç‰‡å…³é”®è¯ï¼Œä¸”è¿˜æœ‰æœªåŒ¹é…çš„å›¾ç‰‡ï¼Œå°è¯•å…³è”ç¬¬ä¸€ä¸ªæœªåŒ¹é…çš„å›¾ç‰‡
                        if unmatched_images:
                            img = unmatched_images.pop(0)
                            
                            # è·å–å‰åæ®µè½æ–‡æœ¬ï¼ˆå¢å¼ºç‰ˆï¼šå¤šæ®µè½ï¼‰
                            prev_paras_text = []
                            next_paras_text = []
                            for i in range(max(0, para_idx - 2), para_idx):
                                if i < len(doc.paragraphs):
                                    prev_text = doc.paragraphs[i].text.strip()
                                    if prev_text:
                                        prev_paras_text.append(prev_text)
                            for i in range(para_idx + 1, min(para_idx + 3, len(doc.paragraphs))):
                                if i < len(doc.paragraphs):
                                    next_text = doc.paragraphs[i].text.strip()
                                    if next_text:
                                        next_paras_text.append(next_text)
                            
                            # è·å–æœ€è¿‘çš„ç« èŠ‚æ ‡é¢˜
                            nearest_section_title = ""
                            for section_idx, section_title in reversed(section_titles):
                                if section_idx <= para_idx:
                                    nearest_section_title = section_title
                                    break
                            
                            # ä½¿ç”¨å¢å¼ºçš„æè¿°ç”Ÿæˆå‡½æ•°
                            description = WordDocumentService._generate_image_description(
                                text, prev_paras_text, next_paras_text, nearest_section_title
                            )
                            
                            # è®¡ç®—ç›¸å¯¹ä½ç½®
                            total_paragraphs = len(doc.paragraphs)
                            relative_position = para_idx / total_paragraphs if total_paragraphs > 0 else 0.0
                            
                            img["position"] = para_idx
                            img["description"] = description
                            img["context"] = text[:300] if text else ""
                            img["prev_context"] = " | ".join(prev_paras_text[:2])[:200] if prev_paras_text else ""
                            img["next_context"] = " | ".join(next_paras_text[:2])[:200] if next_paras_text else ""
                            img["section_title"] = nearest_section_title
                            img["relative_position"] = relative_position
                            img["match_method"] = "keyword"
                            img["match_confidence"] = 0.6  # å…³é”®è¯åŒ¹é…çš„ç½®ä¿¡åº¦
                            
                            # æ·»åŠ åˆ°å½“å‰æ®µè½çš„å›¾ç‰‡åˆ—è¡¨
                            images_in_para = [img]
                            logger.info(f"ğŸ” å›¾ç‰‡ {img.get('image_id')} é€šè¿‡å…³é”®è¯åŒ¹é…åˆ°æ®µè½ {para_idx}ï¼ˆç½®ä¿¡åº¦: 0.6ï¼‰")
                
                # åˆå¹¶OLEå¯¹è±¡åˆ°å½“å‰æ®µè½
                ole_in_para = [obj for obj in ole_objects]
                
                # å¦‚æœæœ‰æ–‡æœ¬ï¼Œæ·»åŠ åˆ°ç»“æ„åŒ–å†…å®¹
                if text:
                    # æ·»åŠ åˆ°ç»“æ„åŒ–å†…å®¹
                    result["structured_content"].append({
                        "type": "heading" if is_heading else "paragraph",
                        "level": heading_level if is_heading else 0,
                        "text": text,
                        "links": links,
                        "images": images_in_para,  # å…³è”å›¾ç‰‡
                        "ole_objects": ole_in_para  # å…³è”OLEå¯¹è±¡
                    })
                    
                    # æ·»åŠ åˆ°çº¯æ–‡æœ¬
                    result["text_content"] += text + "\n"
                    
                    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡æè¿°åˆ°æ–‡æœ¬
                    if images_in_para:
                        for img in images_in_para:
                            result["text_content"] += f"\n[å›¾ç‰‡: {img.get('description', 'å›¾ç‰‡')}]\n"
                    
                    # å¦‚æœæœ‰OLEå¯¹è±¡ï¼Œæ·»åŠ åµŒå…¥æ–‡æ¡£æè¿°åˆ°æ–‡æœ¬
                    if ole_in_para:
                        for ole in ole_in_para:
                            result["text_content"] += f"\n[åµŒå…¥æ–‡æ¡£: {ole.get('name', 'æ–‡æ¡£')} ({ole.get('type', 'æœªçŸ¥ç±»å‹')})]\n"
                
                # å¦‚æœæ®µè½æ²¡æœ‰æ–‡æœ¬ä½†æœ‰å›¾ç‰‡æˆ–OLEå¯¹è±¡ï¼Œå•ç‹¬è®°å½•
                elif images_in_para or ole_in_para:
                    result["structured_content"].append({
                        "type": "image_only" if images_in_para else "ole_only",
                        "level": 0,
                        "text": "",
                        "links": links,
                        "images": images_in_para,
                        "ole_objects": ole_in_para
                    })
                    for img in images_in_para:
                        result["text_content"] += f"\n[å›¾ç‰‡: {img.get('description', 'å›¾ç‰‡')}]\n"
                    for ole in ole_in_para:
                        result["text_content"] += f"\n[åµŒå…¥æ–‡æ¡£: {ole.get('name', 'æ–‡æ¡£')} ({ole.get('type', 'æœªçŸ¥ç±»å‹')})]\n"
                
                para_idx += 1  # å¢åŠ æ®µè½ç´¢å¼•
            
            elif isinstance(element, CT_Tbl):
                # è¡¨æ ¼ - æŒ‰æ–‡æ¡£é¡ºåºåˆ†é…table_id
                table = Table(element, doc)
                table_data = WordDocumentService._extract_table_data(table)
                
                # åˆ†é…å›ºå®šçš„table_idï¼ˆæŒ‰æ–‡æ¡£é¡ºåºï¼‰
                table_counter += 1
                table_id = f"table_{table_counter}"
                table_data["table_id"] = table_id
                
                result["tables"].append(table_data)
                
                # å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
                table_text = WordDocumentService._format_table_as_text(table_data)
                result["text_content"] += table_text + "\n"
                result["structured_content"].append({
                    "type": "table",
                    "data": table_data,
                    "text": table_text,
                    "table_id": table_id  # ä¿å­˜table_id
                })
        
        # å¤„ç†ä»æœªåŒ¹é…çš„å›¾ç‰‡ï¼ˆæœ€åçš„å¤‡ç”¨ç­–ç•¥ï¼‰
        if unmatched_images:
            logger.warning(f"ä»æœ‰ {len(unmatched_images)} å¼ å›¾ç‰‡æœªåŒ¹é…åˆ°ä½ç½®ï¼Œå°†å…³è”åˆ°æ–‡æ¡£æœ«å°¾")
            # å°†è¿™äº›å›¾ç‰‡å…³è”åˆ°æœ€åä¸€ä¸ªæ®µè½ï¼ˆä½œä¸ºæœ€åçš„å¤‡ç”¨ç­–ç•¥ï¼‰
            last_para_idx = para_idx - 1 if para_idx > 0 else 0
            for img in unmatched_images:
                img["position"] = last_para_idx
                logger.warning(f"å›¾ç‰‡ {img.get('image_id')} æœªåŒ¹é…åˆ°ä½ç½®ï¼Œå·²å…³è”åˆ°æ®µè½ {last_para_idx}ï¼ˆå¤‡ç”¨ç­–ç•¥ï¼‰")
                # ç¡®ä¿è¿™äº›å›¾ç‰‡ä¹Ÿè¢«æ·»åŠ åˆ°image_position_mapä¸­
                if last_para_idx not in image_position_map:
                    image_position_map[last_para_idx] = []
                image_position_map[last_para_idx].append(img)
                # åŒæ—¶æ·»åŠ åˆ°æœ€åä¸€ä¸ªstructured_contenté¡¹ä¸­ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if result["structured_content"]:
                    last_item = result["structured_content"][-1]
                    if "images" not in last_item:
                        last_item["images"] = []
                    last_item["images"].append(img)
                    # ä¹Ÿæ·»åŠ åˆ°text_content
                    result["text_content"] += f"\n[å›¾ç‰‡: {img.get('description', 'å›¾ç‰‡')}]\n"
        
        return result
    
    @staticmethod
    def _extract_ole_objects_from_paragraph(paragraph, para_idx: int, document_id: str = None, file_path: str = None) -> List[Dict]:
        """ä»æ®µè½ä¸­æå–OLEå¯¹è±¡ï¼ˆåµŒå…¥æ–‡æ¡£ï¼‰ï¼Œæ”¯æŒMicrosoft Officeå’ŒWPSæ ¼å¼"""
        ole_objects = []
        seen_r_ids = set()  # ç”¨äºå»é‡ï¼Œé¿å…åŒä¸€ä¸ªr_idè¢«å¤šæ¬¡æå–
        try:
            from docx.oxml.ns import qn
            import xml.etree.ElementTree as ET
            
            # éå†æ®µè½ä¸­çš„æ‰€æœ‰runs
            for run in paragraph.runs:
                # æ–¹æ³•1ï¼šæ£€æŸ¥æ˜¯å¦æœ‰OLEObjectå…ƒç´ ï¼ˆMicrosoft Officeæ ¼å¼ï¼‰
                # OLEObject åœ¨ o: å‘½åç©ºé—´ä¸­
                try:
                    ole_elements = run._element.xpath('.//o:OLEObject', namespaces={
                        'o': 'urn:schemas-microsoft-com:office:office'
                    })
                except:
                    # å¦‚æœå‘½åç©ºé—´å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨local-name
                    ole_elements = run._element.xpath('.//*[local-name()="OLEObject"]')
                
                for ole_element in ole_elements:
                    # è·å–OLEå¯¹è±¡ä¿¡æ¯
                    prog_id = ole_element.get('ProgId', '')
                    ole_type = ole_element.get('Type', '')
                    r_id = ole_element.get(qn('r:id'))
                    
                    # å»é‡ï¼šå¦‚æœè¿™ä¸ªr_idå·²ç»è¢«å¤„ç†è¿‡ï¼Œè·³è¿‡
                    if r_id and r_id in seen_r_ids:
                        logger.debug(f"è·³è¿‡é‡å¤çš„OLEå¯¹è±¡: r_id={r_id}, æ®µè½={para_idx}")
                        continue
                    
                    if r_id:
                        seen_r_ids.add(r_id)
                    
                    # ä»å…³ç³»IDè·å–åµŒå…¥æ–‡æ¡£ä¿¡æ¯
                    file_name = ""
                    file_type = "æœªçŸ¥ç±»å‹"
                    
                    if r_id:
                        try:
                            rel = paragraph.part.rels[r_id]
                            if hasattr(rel, 'target_ref'):
                                file_name = rel.target_ref
                            else:
                                file_name = str(rel.target)
                            
                            # æ ¹æ®ProgIdåˆ¤æ–­æ–‡ä»¶ç±»å‹
                            if 'Excel' in prog_id or 'excel' in prog_id.lower():
                                file_type = "Excelæ–‡ä»¶"
                            elif 'Word' in prog_id or 'word' in prog_id.lower():
                                file_type = "Wordæ–‡æ¡£"
                            elif 'PowerPoint' in prog_id or 'powerpoint' in prog_id.lower():
                                file_type = "PowerPointæ¼”ç¤ºæ–‡ç¨¿"
                            elif 'PDF' in prog_id or 'pdf' in prog_id.lower():
                                file_type = "PDFæ–‡æ¡£"
                            else:
                                file_type = prog_id or "åµŒå…¥å¯¹è±¡"
                            
                            # å¦‚æœæ²¡æœ‰æ–‡ä»¶åï¼Œä½¿ç”¨ProgId
                            if not file_name:
                                file_name = prog_id or "åµŒå…¥æ–‡æ¡£"
                        except (KeyError, AttributeError) as e:
                            logger.debug(f"æå–OLEå¯¹è±¡å…³ç³»å¤±è´¥: {e}, r_id={r_id}")
                            file_name = prog_id or "åµŒå…¥æ–‡æ¡£"
                    
                    # å°è¯•æå–å¹¶ä¿å­˜åµŒå…¥æ–‡æ¡£ï¼ˆä¸WPSæ ¼å¼ç›¸åŒçš„é€»è¾‘ï¼‰
                    saved_file_path = None
                    relative_path = None
                    actual_ext = '.bin'
                    temp_ole_id = None
                    
                    if r_id and file_path and document_id:
                        try:
                            rel = paragraph.part.rels[r_id]
                            # ä»docxçš„zipæ–‡ä»¶ä¸­æå–åµŒå…¥æ–‡æ¡£
                            import zipfile
                            
                            # è·å–åµŒå…¥æ–‡æ¡£è·¯å¾„
                            embed_path = None
                            if hasattr(rel, 'target_ref'):
                                embed_path = rel.target_ref
                            elif hasattr(rel, 'target'):
                                embed_path = str(rel.target)
                            
                            if embed_path:
                                with zipfile.ZipFile(file_path, 'r') as zip_file:
                                    # å°è¯•ä¸åŒçš„è·¯å¾„æ ¼å¼
                                    possible_paths = [
                                        embed_path,
                                        f"word/{embed_path}",
                                        f"word/embeddings/{os.path.basename(embed_path)}",
                                        embed_path.replace('../', ''),
                                        embed_path.replace('embeddings/', 'word/embeddings/'),
                                    ]
                                    
                                    found_path = None
                                    for path in possible_paths:
                                        if path in zip_file.namelist():
                                            found_path = path
                                            break
                                    
                                    if found_path:
                                        # åˆ›å»ºä¿å­˜ç›®å½•
                                        ole_dir = os.path.abspath(f"uploads/extracted_ole/{document_id}")
                                        os.makedirs(ole_dir, exist_ok=True)
                                        
                                        # å…ˆè¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ£€æµ‹å®é™…æ ¼å¼
                                        with zip_file.open(found_path) as source:
                                            file_content = source.read()
                                        
                                        # æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼ˆå¯èƒ½ä¼šæå–å¹¶ä¿å­˜æ–‡ä»¶ï¼‰
                                        temp_ole_id = f"ole_temp_{para_idx}_{len(ole_objects)}"
                                        actual_ext = WordDocumentService._detect_file_format(file_content, found_path, ole_dir, temp_ole_id, prog_id)
                                        
                                        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¿å­˜äº†æå–çš„æ–‡ä»¶ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
                                        extracted_file = os.path.join(ole_dir, f"{temp_ole_id}{actual_ext}")
                                        
                                        if os.path.exists(extracted_file) and actual_ext in ['.xls', '.xlsx', '.doc', '.docx', '.ppt', '.pptx']:
                                            # æ–‡ä»¶å·²ç»ç”±_detect_file_formatæå–å¹¶ä¿å­˜ä¸ºæ ‡å‡†æ ¼å¼
                                            saved_file_path = extracted_file
                                            relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{actual_ext}"
                                            logger.info(f"âœ“ ä½¿ç”¨å·²æå–çš„æ ‡å‡†æ ¼å¼æ–‡ä»¶: {saved_file_path} (æ ¼å¼: {actual_ext})")
                                        elif actual_ext in ['.xls', '.xlsx', '.doc', '.docx', '.ppt', '.pptx']:
                                            # _detect_file_formatè¿”å›äº†æ ‡å‡†æ ¼å¼ï¼Œä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯æå–å¤±è´¥ï¼‰
                                            logger.warning(f"æ£€æµ‹åˆ°æ ¼å¼ä¸º{actual_ext}ï¼Œä½†æå–çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•å¼ºåˆ¶æå–...")
                                            retry_temp_id = f"{temp_ole_id}_retry"
                                            retry_ext = WordDocumentService._detect_file_format(file_content, found_path, ole_dir, retry_temp_id, prog_id)
                                            retry_file = os.path.join(ole_dir, f"{retry_temp_id}{retry_ext}")
                                            
                                            if os.path.exists(retry_file) and retry_ext in ['.xls', '.xlsx', '.doc', '.docx', '.ppt', '.pptx']:
                                                saved_file_path = os.path.join(ole_dir, f"{temp_ole_id}{retry_ext}")
                                                os.rename(retry_file, saved_file_path)
                                                relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{retry_ext}"
                                                actual_ext = retry_ext
                                                logger.info(f"âœ“ å¼ºåˆ¶æå–æˆåŠŸ: {saved_file_path} (æ ¼å¼: {retry_ext})")
                                            else:
                                                # æå–å¤±è´¥ï¼Œä¿å­˜ä¸º.binæ ¼å¼
                                                actual_ext = '.bin'
                                                saved_file_path = os.path.join(ole_dir, f"{temp_ole_id}{actual_ext}")
                                                relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{actual_ext}"
                                                with open(saved_file_path, 'wb') as target:
                                                    target.write(file_content)
                                                logger.warning(f"âœ— æå–å¤±è´¥ï¼Œä¿å­˜ä¸ºåŸå§‹.binæ ¼å¼: {saved_file_path}")
                                        else:
                                            # è¿”å›çš„æ˜¯.binæˆ–å…¶ä»–æ ¼å¼ï¼Œç›´æ¥ä¿å­˜åŸå§‹æ–‡ä»¶
                                            saved_file_path = os.path.join(ole_dir, f"{temp_ole_id}{actual_ext}")
                                            relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{actual_ext}"
                                            
                                            with open(saved_file_path, 'wb') as target:
                                                target.write(file_content)
                                            
                                            logger.info(f"ä¿å­˜åµŒå…¥æ–‡æ¡£: {saved_file_path} (æ¥æº: {found_path}, æ ¼å¼: {actual_ext})")
                                    else:
                                        logger.debug(f"åœ¨zipæ–‡ä»¶ä¸­æœªæ‰¾åˆ°åµŒå…¥æ–‡æ¡£è·¯å¾„: {embed_path}, å°è¯•çš„è·¯å¾„: {possible_paths}")
                        except Exception as e:
                            logger.warning(f"æå–åµŒå…¥æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
                    
                    # è·å–ä¸Šä¸‹æ–‡ï¼ˆæ®µè½æ–‡æœ¬ï¼‰
                    para_text = paragraph.text.strip()
                    
                    # æ›´æ–°æ–‡ä»¶åï¼Œä½¿ç”¨å®é™…ä¿å­˜çš„æ‰©å±•å
                    if actual_ext and actual_ext != '.bin':
                        base_name = os.path.splitext(file_name)[0] if file_name else "åµŒå…¥æ–‡æ¡£"
                        display_name = f"{base_name}{actual_ext}"
                    else:
                        display_name = file_name
                    
                    # ole_idå°†åœ¨è°ƒç”¨å¤„æŒ‰æ–‡æ¡£é¡ºåºåˆ†é…ï¼Œè¿™é‡Œå…ˆä¸è®¾ç½®
                    ole_objects.append({
                        "position": para_idx,
                        "name": display_name,  # ä½¿ç”¨æ›´æ–°åçš„æ–‡ä»¶å
                        "type": file_type,
                        "prog_id": prog_id,
                        "ole_type": ole_type,
                        "context": para_text[:100] if para_text else "",
                        "file_path": saved_file_path,
                        "relative_path": relative_path,
                        "temp_ole_id": temp_ole_id,  # ä¿å­˜ä¸´æ—¶IDï¼Œç”¨äºåç»­é‡å‘½åæ–‡ä»¶
                        "file_ext": actual_ext  # ä¿å­˜æ–‡ä»¶æ‰©å±•å
                    })
                
                # æ–¹æ³•2ï¼šæ£€æŸ¥æ˜¯å¦æœ‰Objectå…ƒç´ ï¼ˆå¦ä¸€ç§åµŒå…¥æ–¹å¼ï¼‰
                try:
                    object_elements = run._element.xpath('.//o:Object', namespaces={
                        'o': 'urn:schemas-microsoft-com:office:office'
                    })
                except:
                    object_elements = run._element.xpath('.//*[local-name()="Object"]')
                
                # æ–¹æ³•3ï¼šæ£€æŸ¥WPSæ ¼å¼çš„åµŒå…¥å¯¹è±¡ï¼ˆé€šè¿‡XMLå†…å®¹æ£€æŸ¥ï¼‰
                try:
                    run_xml = run._element.xml
                    if run_xml:
                        # WPSå¯èƒ½ä½¿ç”¨ä¸åŒçš„æ ‡ç­¾æˆ–å‘½åç©ºé—´
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«Excelç›¸å…³çš„å…³é”®è¯
                        if any(keyword in run_xml.lower() for keyword in ['excel', 'xls', 'xlsx', 'spreadsheet']):
                            # å°è¯•ä»XMLä¸­æå–ä¿¡æ¯
                            root = ET.fromstring(run_xml)
                            for elem in root.iter():
                                tag = elem.tag.lower()
                                if 'excel' in tag or 'ole' in tag or 'object' in tag or 'embed' in tag:
                                    # æå–ProgIdæˆ–ç±»å‹ä¿¡æ¯
                                    prog_id = elem.get('ProgId', '') or elem.get('progid', '')
                                    if not prog_id:
                                        # å°è¯•ä»å±æ€§ä¸­è·å–
                                        for attr_name, attr_value in elem.attrib.items():
                                            if 'excel' in attr_value.lower() or 'xls' in attr_value.lower():
                                                prog_id = attr_value
                                                break
                                    
                                    if prog_id or 'excel' in tag or 'xls' in tag:
                                        # è·å–å…³ç³»ID
                                        r_id = None
                                        for attr_name, attr_value in elem.attrib.items():
                                            if 'id' in attr_name.lower() and attr_value:
                                                try:
                                                    r_id = elem.get(qn('r:id')) or attr_value
                                                    break
                                                except:
                                                    r_id = attr_value
                                                    break
                                        
                                        # å»é‡ï¼šå¦‚æœè¿™ä¸ªr_idå·²ç»è¢«å¤„ç†è¿‡ï¼Œè·³è¿‡
                                        if r_id and r_id in seen_r_ids:
                                            logger.debug(f"è·³è¿‡é‡å¤çš„WPSæ ¼å¼OLEå¯¹è±¡: r_id={r_id}, æ®µè½={para_idx}")
                                            continue
                                        
                                        if r_id:
                                            seen_r_ids.add(r_id)
                                        
                                        file_name = ""
                                        file_type = "Excelæ–‡ä»¶"
                                        
                                        if r_id:
                                            try:
                                                rel = paragraph.part.rels[r_id]
                                                if hasattr(rel, 'target_ref'):
                                                    file_name = rel.target_ref
                                                else:
                                                    file_name = str(rel.target) if hasattr(rel, 'target') else ""
                                            except:
                                                pass
                                        
                                        if not file_name:
                                            file_name = prog_id or "Excelæ–‡ä»¶"
                                        
                                        # å°è¯•æå–å¹¶ä¿å­˜åµŒå…¥æ–‡æ¡£
                                        saved_file_path = None
                                        relative_path = None
                                        if r_id and file_path and document_id:
                                            try:
                                                rel = paragraph.part.rels[r_id]
                                                # ä»docxçš„zipæ–‡ä»¶ä¸­æå–åµŒå…¥æ–‡æ¡£
                                                import zipfile
                                                import shutil
                                                
                                                # è·å–åµŒå…¥æ–‡æ¡£è·¯å¾„
                                                embed_path = None
                                                if hasattr(rel, 'target_ref'):
                                                    embed_path = rel.target_ref
                                                elif hasattr(rel, 'target'):
                                                    embed_path = str(rel.target)
                                                
                                                if embed_path:
                                                    with zipfile.ZipFile(file_path, 'r') as zip_file:
                                                        # å°è¯•ä¸åŒçš„è·¯å¾„æ ¼å¼
                                                        possible_paths = [
                                                            embed_path,
                                                            f"word/{embed_path}",
                                                            f"word/embeddings/{os.path.basename(embed_path)}",
                                                            embed_path.replace('../', ''),
                                                            embed_path.replace('embeddings/', 'word/embeddings/'),
                                                        ]
                                                        
                                                        found_path = None
                                                        for path in possible_paths:
                                                            if path in zip_file.namelist():
                                                                found_path = path
                                                                break
                                                        
                                                        if found_path:
                                                            # åˆ›å»ºä¿å­˜ç›®å½•
                                                            ole_dir = os.path.abspath(f"uploads/extracted_ole/{document_id}")
                                                            os.makedirs(ole_dir, exist_ok=True)
                                                            
                                                            # å…ˆè¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ£€æµ‹å®é™…æ ¼å¼
                                                            with zip_file.open(found_path) as source:
                                                                file_content = source.read()
                                                            
                                                            # æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼ˆå¯èƒ½ä¼šæå–å¹¶ä¿å­˜æ–‡ä»¶ï¼‰
                                                            # ole_idå°†åœ¨è°ƒç”¨å¤„æŒ‰æ–‡æ¡£é¡ºåºåˆ†é…ï¼Œè¿™é‡Œä½¿ç”¨ä¸´æ—¶IDç”¨äºæ–‡ä»¶ä¿å­˜
                                                            temp_ole_id = f"ole_temp_{para_idx}_{len(ole_objects)}"
                                                            actual_ext = WordDocumentService._detect_file_format(file_content, found_path, ole_dir, temp_ole_id, prog_id)
                                                            
                                                            # æ£€æŸ¥æ˜¯å¦å·²ç»ä¿å­˜äº†æå–çš„æ–‡ä»¶ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
                                                            extracted_file = os.path.join(ole_dir, f"{temp_ole_id}{actual_ext}")
                                                            
                                                            if os.path.exists(extracted_file) and actual_ext in ['.xls', '.xlsx', '.doc', '.docx', '.ppt', '.pptx']:
                                                                # æ–‡ä»¶å·²ç»ç”±_detect_file_formatæå–å¹¶ä¿å­˜ä¸ºæ ‡å‡†æ ¼å¼
                                                                saved_file_path = extracted_file
                                                                relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{actual_ext}"
                                                                logger.info(f"âœ“ ä½¿ç”¨å·²æå–çš„æ ‡å‡†æ ¼å¼æ–‡ä»¶: {saved_file_path} (æ ¼å¼: {actual_ext})")
                                                            elif actual_ext in ['.xls', '.xlsx', '.doc', '.docx', '.ppt', '.pptx']:
                                                                # _detect_file_formatè¿”å›äº†æ ‡å‡†æ ¼å¼ï¼Œä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯æå–å¤±è´¥ï¼‰
                                                                # å°è¯•å¼ºåˆ¶æå–ï¼šé‡æ–°è°ƒç”¨_detect_file_formatï¼Œç¡®ä¿æå–æˆåŠŸ
                                                                logger.warning(f"æ£€æµ‹åˆ°æ ¼å¼ä¸º{actual_ext}ï¼Œä½†æå–çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•å¼ºåˆ¶æå–...")
                                                                # é‡æ–°å°è¯•æå–ï¼ˆä½¿ç”¨æ–°çš„ä¸´æ—¶IDé¿å…å†²çªï¼‰
                                                                retry_temp_id = f"{temp_ole_id}_retry"
                                                                retry_ext = WordDocumentService._detect_file_format(file_content, found_path, ole_dir, retry_temp_id, prog_id)
                                                                retry_file = os.path.join(ole_dir, f"{retry_temp_id}{retry_ext}")
                                                                
                                                                if os.path.exists(retry_file) and retry_ext in ['.xls', '.xlsx', '.doc', '.docx', '.ppt', '.pptx']:
                                                                    # é‡å‘½åä¸ºåŸå§‹ID
                                                                    saved_file_path = os.path.join(ole_dir, f"{temp_ole_id}{retry_ext}")
                                                                    os.rename(retry_file, saved_file_path)
                                                                    relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{retry_ext}"
                                                                    logger.info(f"âœ“ å¼ºåˆ¶æå–æˆåŠŸ: {saved_file_path} (æ ¼å¼: {retry_ext})")
                                                                else:
                                                                    # æå–å¤±è´¥ï¼Œä¿å­˜ä¸º.binæ ¼å¼
                                                                    actual_ext = '.bin'
                                                                    saved_file_path = os.path.join(ole_dir, f"{temp_ole_id}{actual_ext}")
                                                                    relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{actual_ext}"
                                                                    with open(saved_file_path, 'wb') as target:
                                                                        target.write(file_content)
                                                                    logger.warning(f"âœ— æå–å¤±è´¥ï¼Œä¿å­˜ä¸ºåŸå§‹.binæ ¼å¼: {saved_file_path}")
                                                            else:
                                                                # è¿”å›çš„æ˜¯.binæˆ–å…¶ä»–æ ¼å¼ï¼Œç›´æ¥ä¿å­˜åŸå§‹æ–‡ä»¶
                                                                saved_file_path = os.path.join(ole_dir, f"{temp_ole_id}{actual_ext}")
                                                                relative_path = f"extracted_ole/{document_id}/{temp_ole_id}{actual_ext}"
                                                                
                                                                with open(saved_file_path, 'wb') as target:
                                                                    target.write(file_content)
                                                                
                                                                logger.info(f"ä¿å­˜åµŒå…¥æ–‡æ¡£: {saved_file_path} (æ¥æº: {found_path}, æ ¼å¼: {actual_ext})")
                                                        else:
                                                            logger.debug(f"åœ¨zipæ–‡ä»¶ä¸­æœªæ‰¾åˆ°åµŒå…¥æ–‡æ¡£è·¯å¾„: {embed_path}, å°è¯•çš„è·¯å¾„: {possible_paths}")
                                            except Exception as e:
                                                logger.warning(f"æå–åµŒå…¥æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
                                        
                                        para_text = paragraph.text.strip()
                                        
                                        # ole_idå°†åœ¨è°ƒç”¨å¤„æŒ‰æ–‡æ¡£é¡ºåºåˆ†é…ï¼Œè¿™é‡Œå…ˆä¸è®¾ç½®
                                        # æ›´æ–°æ–‡ä»¶åï¼Œä½¿ç”¨å®é™…ä¿å­˜çš„æ‰©å±•å
                                        if actual_ext and actual_ext != '.bin':
                                            # å¦‚æœæˆåŠŸæå–ä¸ºæ ‡å‡†æ ¼å¼ï¼Œæ›´æ–°æ–‡ä»¶åæ˜¾ç¤º
                                            base_name = os.path.splitext(file_name)[0] if file_name else "åµŒå…¥æ–‡æ¡£"
                                            display_name = f"{base_name}{actual_ext}"
                                        else:
                                            display_name = file_name
                                        
                                        ole_objects.append({
                                            "position": para_idx,
                                            "name": display_name,  # ä½¿ç”¨æ›´æ–°åçš„æ–‡ä»¶å
                                            "type": file_type,
                                            "prog_id": prog_id or "Excel.Sheet",
                                            "ole_type": "WPS_Embedded",
                                            "context": para_text[:100] if para_text else "",
                                            "file_path": saved_file_path,
                                            "relative_path": relative_path,
                                            "temp_ole_id": temp_ole_id,  # ä¿å­˜ä¸´æ—¶IDï¼Œç”¨äºåç»­é‡å‘½åæ–‡ä»¶
                                            "file_ext": actual_ext  # ä¿å­˜æ–‡ä»¶æ‰©å±•å
                                        })
                                        logger.info(f"ä»æ®µè½ {para_idx} æå–åˆ°WPSæ ¼å¼çš„ExcelåµŒå…¥å¯¹è±¡: {file_name}")
                except Exception as e:
                    logger.debug(f"æ£€æŸ¥WPSæ ¼å¼åµŒå…¥å¯¹è±¡æ—¶å‡ºé”™: {e}")
                
                for obj_element in object_elements:
                    prog_id = obj_element.get('ProgId', '')
                    r_id = obj_element.get(qn('r:id'))
                    
                    # å»é‡ï¼šå¦‚æœè¿™ä¸ªr_idå·²ç»è¢«å¤„ç†è¿‡ï¼Œè·³è¿‡
                    if r_id and r_id in seen_r_ids:
                        logger.debug(f"è·³è¿‡é‡å¤çš„Objectå…ƒç´ OLEå¯¹è±¡: r_id={r_id}, æ®µè½={para_idx}")
                        continue
                    
                    if r_id:
                        seen_r_ids.add(r_id)
                    
                    if prog_id or r_id:
                        file_name = ""
                        file_type = "æœªçŸ¥ç±»å‹"
                        
                        if r_id:
                            try:
                                rel = paragraph.part.rels[r_id]
                                if hasattr(rel, 'target_ref'):
                                    file_name = rel.target_ref
                                else:
                                    file_name = str(rel.target)
                            except (KeyError, AttributeError):
                                file_name = prog_id or "åµŒå…¥æ–‡æ¡£"
                        
                        if 'Excel' in prog_id or 'excel' in prog_id.lower():
                            file_type = "Excelæ–‡ä»¶"
                        elif 'Word' in prog_id or 'word' in prog_id.lower():
                            file_type = "Wordæ–‡æ¡£"
                        else:
                            file_type = prog_id or "åµŒå…¥å¯¹è±¡"
                        
                        para_text = paragraph.text.strip()
                        
                        # ole_idå°†åœ¨è°ƒç”¨å¤„æŒ‰æ–‡æ¡£é¡ºåºåˆ†é…ï¼Œè¿™é‡Œå…ˆä¸è®¾ç½®
                        ole_objects.append({
                            "position": para_idx,
                            "name": file_name or prog_id or "åµŒå…¥æ–‡æ¡£",
                            "type": file_type,
                            "prog_id": prog_id,
                            "ole_type": "Object",
                            "context": para_text[:100] if para_text else ""
                        })
            
            if ole_objects:
                logger.info(f"ä»æ®µè½ {para_idx} æå–åˆ° {len(ole_objects)} ä¸ªOLEå¯¹è±¡")
        except Exception as e:
            logger.warning(f"ä»æ®µè½æå–OLEå¯¹è±¡æ—¶å‡ºé”™: {e}", exc_info=True)
        
        return ole_objects
    
    @staticmethod
    def _extract_links_from_paragraph(paragraph) -> List[Dict]:
        """ä»æ®µè½ä¸­æå–é“¾æ¥"""
        links = []
        try:
            from docx.oxml.ns import qn
            
            # éå†æ®µè½ä¸­çš„æ‰€æœ‰runs
            for run in paragraph.runs:
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶…é“¾æ¥
                hyperlinks = run._element.xpath('.//w:hyperlink')
                
                for hyperlink in hyperlinks:
                    # è·å–é“¾æ¥åœ°å€
                    r_id = hyperlink.get(qn('r:id'))
                    if r_id:
                        # ä»æ–‡æ¡£çš„å…³ç³»ä¸­è·å–é“¾æ¥åœ°å€
                        try:
                            rel = paragraph.part.rels[r_id]
                            url = rel.target_ref if hasattr(rel, 'target_ref') else str(rel.target)
                            
                            # è·å–é“¾æ¥æ–‡æœ¬
                            link_text = run.text.strip()
                            if not link_text:
                                # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œå°è¯•ä»hyperlinkå…ƒç´ ä¸­è·å–
                                link_text = ''.join(hyperlink.itertext()).strip()
                            
                            if url:
                                # åˆ¤æ–­é“¾æ¥ç±»å‹
                                link_type = "external"
                                if url.startswith('#'):
                                    link_type = "internal"
                                elif url.startswith('file://') or url.endswith(('.docx', '.doc', '.xlsx', '.xls', '.pdf')):
                                    link_type = "file"
                                
                                links.append({
                                    "text": link_text or url,  # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œä½¿ç”¨URL
                                    "url": url,
                                    "type": link_type
                                })
                        except (KeyError, AttributeError) as e:
                            logger.debug(f"æå–é“¾æ¥å¤±è´¥: {e}, r_id={r_id}")
                            continue
        except Exception as e:
            logger.warning(f"ä»æ®µè½æå–é“¾æ¥æ—¶å‡ºé”™: {e}", exc_info=True)
        
        return links
    
    @staticmethod
    def _extract_table_data(table) -> Dict[str, Any]:
        """æå–è¡¨æ ¼æ•°æ®"""
        if not table.rows:
            return {"headers": [], "rows": [], "row_count": 0, "col_count": 0}
        
        # æå–è¡¨å¤´ï¼ˆç¬¬ä¸€è¡Œï¼‰
        headers = [cell.text.strip() for cell in table.rows[0].cells]
        
        # æå–æ•°æ®è¡Œ
        rows = []
        for row in table.rows[1:]:
            row_data = [cell.text.strip() for cell in row.cells]
            rows.append(row_data)
        
        return {
            "headers": headers,
            "rows": rows,
            "row_count": len(rows),
            "col_count": len(headers)
        }
    
    @staticmethod
    def _detect_file_format(file_content: bytes, original_path: str, save_dir: str, ole_id: str, prog_id: str = None) -> str:
        """
        æ£€æµ‹æ–‡ä»¶çš„å®é™…æ ¼å¼å¹¶æå–å†…å®¹
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
            original_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            save_dir: ä¿å­˜ç›®å½•
            ole_id: OLEå¯¹è±¡ID
            prog_id: OLEå¯¹è±¡çš„ProgIdï¼ˆç”¨äºè¾…åŠ©åˆ¤æ–­æ–‡ä»¶ç±»å‹ï¼‰
            
        Returns:
            å®é™…çš„æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚ .xlsx, .xls, .docx, .doc, .pptx, .pptç­‰ï¼‰
        """
        if len(file_content) < 8:
            return '.bin'
        
        header = file_content[:8]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ZIPæ ¼å¼ï¼ˆ.xlsx, .docx, .pptxå®é™…ä¸Šæ˜¯ZIPï¼‰
        if header[:2] == b'PK':
            # å°è¯•ä½œä¸ºZIPæ‰“å¼€ï¼Œæ£€æŸ¥æ–‡ä»¶ç±»å‹
            try:
                import zipfile
                import io
                with zipfile.ZipFile(io.BytesIO(file_content), 'r') as zf:
                    file_list = zf.namelist()
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«Excelçš„ç‰¹å¾æ–‡ä»¶
                    if any('xl/' in name or 'xl/workbook' in name or 'xl/worksheets' in name for name in file_list):
                        logger.info(f"æ£€æµ‹åˆ°Excelæ ¼å¼ï¼ˆ.xlsxï¼‰: {ole_id}")
                        return '.xlsx'
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«Wordçš„ç‰¹å¾æ–‡ä»¶
                    elif any('word/' in name or 'word/document' in name for name in file_list):
                        logger.info(f"æ£€æµ‹åˆ°Wordæ ¼å¼ï¼ˆ.docxï¼‰: {ole_id}")
                        return '.docx'
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«PowerPointçš„ç‰¹å¾æ–‡ä»¶
                    elif any('ppt/' in name or 'ppt/presentation' in name or 'ppt/slides' in name for name in file_list):
                        logger.info(f"æ£€æµ‹åˆ°PowerPointæ ¼å¼ï¼ˆ.pptxï¼‰: {ole_id}")
                        return '.pptx'
            except Exception as e:
                logger.debug(f"ZIPæ ¼å¼æ£€æµ‹å¤±è´¥: {e}")
            # å¦‚æœæ˜¯ZIPä½†ä¸æ˜¯å·²çŸ¥çš„Officeæ ¼å¼ï¼Œè¿”å›.zip
            return '.zip'
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯OLE2æ ¼å¼ï¼ˆComposite Document File V2ï¼‰
        # OLE2ç­¾å: D0 CF 11 E0 A1 B1 1A E1
        ole2_signature = b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1'
        if header == ole2_signature:
            # è¿™æ˜¯OLE2æ ¼å¼ï¼Œå°è¯•ä½¿ç”¨olefileåº“æå–å†…å®¹
            try:
                import olefile
                import io
                if olefile.isOleFile(io.BytesIO(file_content)):
                    ole = olefile.OleFileIO(io.BytesIO(file_content))
                    try:
                        # è·å–æ‰€æœ‰æµ
                        stream_list = ole.listdir()
                        
                        # æ ¹æ®ProgIdå’Œæµååˆ¤æ–­æ–‡ä»¶ç±»å‹
                        detected_type = None
                        extracted_data = None
                        stream_name_used = None
                        
                        # è®°å½•æ‰€æœ‰æµåç”¨äºè°ƒè¯•
                        all_streams_debug = []
                        for s in stream_list:
                            if isinstance(s, tuple):
                                all_streams_debug.append('/'.join(s))
                            else:
                                all_streams_debug.append(str(s))
                        logger.info(f"OLE2æ–‡ä»¶ä¸­çš„æ‰€æœ‰æµ: {all_streams_debug}")
                        
                        # æ£€æŸ¥Excelæµ
                        for stream_name in stream_list:
                            if isinstance(stream_name, tuple) and len(stream_name) > 0:
                                stream_name_str = stream_name[0]
                            else:
                                stream_name_str = str(stream_name)
                            
                            if stream_name_str in ['Workbook', 'Book']:
                                detected_type = '.xls'
                                stream_name_used = stream_name_str
                                break
                        
                        # æ£€æŸ¥Wordæµ
                        if not detected_type:
                            for stream_name in stream_list:
                                if isinstance(stream_name, tuple) and len(stream_name) > 0:
                                    stream_name_str = stream_name[0]
                                else:
                                    stream_name_str = str(stream_name)
                                
                                if stream_name_str == 'WordDocument':
                                    detected_type = '.doc'
                                    stream_name_used = stream_name_str
                                    break
                        
                        # æ£€æŸ¥PowerPointæµ
                        if not detected_type:
                            for stream_name in stream_list:
                                if isinstance(stream_name, tuple) and len(stream_name) > 0:
                                    stream_name_str = stream_name[0]
                                else:
                                    stream_name_str = str(stream_name)
                                
                                if 'PowerPoint' in stream_name_str or 'Presentation' in stream_name_str:
                                    detected_type = '.ppt'
                                    stream_name_used = stream_name_str
                                    break
                        
                        # å¦‚æœé€šè¿‡æµåæ— æ³•åˆ¤æ–­ï¼Œå°è¯•æ ¹æ®ProgIdåˆ¤æ–­
                        if not detected_type and prog_id:
                            prog_id_lower = prog_id.lower()
                            if 'excel' in prog_id_lower:
                                detected_type = '.xls'
                            elif 'word' in prog_id_lower:
                                detected_type = '.doc'
                            elif 'powerpoint' in prog_id_lower or 'ppt' in prog_id_lower:
                                detected_type = '.ppt'
                        
                        # å¦‚æœæ‰¾åˆ°äº†ç±»å‹ï¼Œæå–å†…å®¹
                        if detected_type and stream_name_used:
                            try:
                                # æ‰¾åˆ°å¯¹åº”çš„æµå¯¹è±¡ï¼ˆstream_name_usedæ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦æ‰¾åˆ°å®é™…çš„æµå¯¹è±¡ï¼‰
                                actual_stream_name = None
                                for stream_name in stream_list:
                                    if isinstance(stream_name, tuple) and len(stream_name) > 0:
                                        stream_name_str = stream_name[0]
                                    else:
                                        stream_name_str = str(stream_name)
                                    
                                    if stream_name_str == stream_name_used:
                                        actual_stream_name = stream_name
                                        break
                                
                                if actual_stream_name:
                                    extracted_data = ole.openstream(actual_stream_name).read()
                                    # ä¿å­˜æå–çš„å†…å®¹
                                    extracted_path = os.path.join(save_dir, f"{ole_id}{detected_type}")
                                    with open(extracted_path, 'wb') as f:
                                        f.write(extracted_data)
                                    logger.info(f"âœ“ æˆåŠŸä»OLE2æ ¼å¼æå–{detected_type}å†…å®¹: {extracted_path}, æµå: {stream_name_used}")
                                    ole.close()
                                    return detected_type
                                else:
                                    logger.warning(f"âœ— æ‰¾ä¸åˆ°æµå¯¹è±¡: {stream_name_used}, å¯ç”¨æµ: {all_streams_debug}")
                            except Exception as e:
                                logger.warning(f"âœ— æå–{detected_type}å†…å®¹å¤±è´¥: {e}", exc_info=True)
                        
                        # å¦‚æœæ‰¾ä¸åˆ°æ ‡å‡†æµï¼Œæ£€æŸ¥æ˜¯å¦æœ‰packageæµï¼ˆæ‰“åŒ…çš„OLEå¯¹è±¡ï¼‰
                        # å³ä½¿detected_typeå­˜åœ¨ï¼Œå¦‚æœæ²¡æœ‰stream_name_usedï¼Œä¹Ÿè¦æ£€æŸ¥packageæµ
                        if not stream_name_used:
                            package_stream = None
                            logger.info(f"å¼€å§‹æŸ¥æ‰¾packageæµï¼Œæµåˆ—è¡¨ç±»å‹: {type(stream_list)}, æ•°é‡: {len(stream_list)}")
                            for idx, stream_name in enumerate(stream_list):
                                # å¤„ç†æµåï¼ˆå¯èƒ½æ˜¯tupleæˆ–å­—ç¬¦ä¸²ï¼‰
                                stream_first_str = None
                                stream_type_info = f"ç±»å‹: {type(stream_name)}"
                                
                                if isinstance(stream_name, tuple):
                                    stream_type_info += f", é•¿åº¦: {len(stream_name)}"
                                    if len(stream_name) > 0:
                                        stream_first = stream_name[0]
                                        stream_type_info += f", ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(stream_first)}"
                                        # å¤„ç†ä¸åŒç±»å‹çš„æµå
                                        if isinstance(stream_first, bytes):
                                            try:
                                                stream_first_str = stream_first.decode('utf-8', errors='ignore').strip()
                                            except:
                                                stream_first_str = str(stream_first).strip()
                                        elif isinstance(stream_first, str):
                                            stream_first_str = stream_first.strip()
                                        else:
                                            stream_first_str = str(stream_first).strip()
                                else:
                                    if isinstance(stream_name, bytes):
                                        try:
                                            stream_first_str = stream_name.decode('utf-8', errors='ignore').strip()
                                        except:
                                            stream_first_str = str(stream_name).strip()
                                    else:
                                        stream_first_str = str(stream_name).strip()
                                
                                logger.info(f"æµ[{idx}]: {stream_name} -> {stream_first_str} ({stream_type_info})")
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯packageæµï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œå»é™¤ç©ºç™½å­—ç¬¦ï¼‰
                                # ä¹Ÿæ£€æŸ¥æµåçš„å­—ç¬¦ä¸²è¡¨ç¤ºä¸­æ˜¯å¦åŒ…å«'package'
                                if stream_first_str:
                                    stream_lower = stream_first_str.lower()
                                    stream_repr = repr(stream_name).lower()
                                    if stream_lower == 'package' or 'package' in stream_lower or 'package' in stream_repr:
                                        package_stream = stream_name
                                        logger.info(f"âœ“âœ“âœ“ æ‰¾åˆ°packageæµ: {package_stream} (åŸå§‹: {stream_first_str}, repr: {stream_repr})")
                                        break
                            
                            if package_stream:
                                logger.info(f"æ‰¾åˆ°packageæµï¼Œå°è¯•æå–å†…å®¹: {package_stream}")
                                try:
                                    package_data = ole.openstream(package_stream).read()
                                    
                                    # æ£€æŸ¥æå–çš„æ•°æ®æ ¼å¼
                                    if len(package_data) >= 2 and package_data[:2] == b'PK':
                                        # ZIPæ ¼å¼ï¼Œå¯èƒ½æ˜¯.xlsxã€.docxæˆ–.pptxæ–‡ä»¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­
                                        try:
                                            import zipfile
                                            import io
                                            with zipfile.ZipFile(io.BytesIO(package_data), 'r') as zf:
                                                file_list = zf.namelist()
                                                # æ£€æŸ¥æ˜¯å¦åŒ…å«Excelçš„ç‰¹å¾æ–‡ä»¶
                                                if any('xl/' in name or 'xl/workbook' in name or 'xl/worksheets' in name for name in file_list):
                                                    extracted_path = os.path.join(save_dir, f"{ole_id}.xlsx")
                                                    with open(extracted_path, 'wb') as f:
                                                        f.write(package_data)
                                                    logger.info(f"ä»packageæµæå–åˆ°.xlsxæ ¼å¼æ–‡ä»¶: {extracted_path}")
                                                    ole.close()
                                                    return '.xlsx'
                                                # æ£€æŸ¥æ˜¯å¦åŒ…å«Wordçš„ç‰¹å¾æ–‡ä»¶
                                                elif any('word/' in name or 'word/document' in name for name in file_list):
                                                    extracted_path = os.path.join(save_dir, f"{ole_id}.docx")
                                                    with open(extracted_path, 'wb') as f:
                                                        f.write(package_data)
                                                    logger.info(f"ä»packageæµæå–åˆ°.docxæ ¼å¼æ–‡ä»¶: {extracted_path}")
                                                    ole.close()
                                                    return '.docx'
                                                # æ£€æŸ¥æ˜¯å¦åŒ…å«PowerPointçš„ç‰¹å¾æ–‡ä»¶
                                                elif any('ppt/' in name or 'ppt/presentation' in name or 'ppt/slides' in name for name in file_list):
                                                    extracted_path = os.path.join(save_dir, f"{ole_id}.pptx")
                                                    with open(extracted_path, 'wb') as f:
                                                        f.write(package_data)
                                                    logger.info(f"ä»packageæµæå–åˆ°.pptxæ ¼å¼æ–‡ä»¶: {extracted_path}")
                                                    ole.close()
                                                    return '.pptx'
                                                else:
                                                    # æ— æ³•è¯†åˆ«ï¼Œæ ¹æ®ProgIdåˆ¤æ–­
                                                    if prog_id:
                                                        prog_id_lower = prog_id.lower()
                                                        if 'excel' in prog_id_lower:
                                                            extracted_path = os.path.join(save_dir, f"{ole_id}.xlsx")
                                                            with open(extracted_path, 'wb') as f:
                                                                f.write(package_data)
                                                            logger.info(f"ä»packageæµæå–æ•°æ®ï¼Œæ ¹æ®ProgIdåˆ¤æ–­ä¸º.xlsx: {extracted_path}")
                                                            ole.close()
                                                            return '.xlsx'
                                                        elif 'word' in prog_id_lower:
                                                            extracted_path = os.path.join(save_dir, f"{ole_id}.docx")
                                                            with open(extracted_path, 'wb') as f:
                                                                f.write(package_data)
                                                            logger.info(f"ä»packageæµæå–æ•°æ®ï¼Œæ ¹æ®ProgIdåˆ¤æ–­ä¸º.docx: {extracted_path}")
                                                            ole.close()
                                                            return '.docx'
                                                        elif 'powerpoint' in prog_id_lower or 'ppt' in prog_id_lower:
                                                            extracted_path = os.path.join(save_dir, f"{ole_id}.pptx")
                                                            with open(extracted_path, 'wb') as f:
                                                                f.write(package_data)
                                                            logger.info(f"ä»packageæµæå–æ•°æ®ï¼Œæ ¹æ®ProgIdåˆ¤æ–­ä¸º.pptx: {extracted_path}")
                                                            ole.close()
                                                            return '.pptx'
                                                    # å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œé»˜è®¤ä¿å­˜ä¸º.xlsxï¼ˆå‘åå…¼å®¹ï¼‰
                                                    extracted_path = os.path.join(save_dir, f"{ole_id}.xlsx")
                                                    with open(extracted_path, 'wb') as f:
                                                        f.write(package_data)
                                                    logger.warning(f"ä»packageæµæå–ZIPæ•°æ®ï¼Œæ— æ³•è¯†åˆ«æ ¼å¼ï¼Œé»˜è®¤ä¿å­˜ä¸º.xlsx: {extracted_path}")
                                                    ole.close()
                                                    return '.xlsx'
                                        except Exception as e:
                                            logger.warning(f"æ£€æŸ¥ZIPæ–‡ä»¶å†…å®¹å¤±è´¥: {e}ï¼Œæ ¹æ®ProgIdåˆ¤æ–­")
                                            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œæ ¹æ®ProgIdåˆ¤æ–­
                                            if prog_id:
                                                prog_id_lower = prog_id.lower()
                                                if 'word' in prog_id_lower:
                                                    extracted_path = os.path.join(save_dir, f"{ole_id}.docx")
                                                    with open(extracted_path, 'wb') as f:
                                                        f.write(package_data)
                                                    logger.info(f"ä»packageæµæå–æ•°æ®ï¼Œæ ¹æ®ProgIdåˆ¤æ–­ä¸º.docx: {extracted_path}")
                                                    ole.close()
                                                    return '.docx'
                                            # é»˜è®¤ä¿å­˜ä¸º.xlsx
                                            extracted_path = os.path.join(save_dir, f"{ole_id}.xlsx")
                                            with open(extracted_path, 'wb') as f:
                                                f.write(package_data)
                                            logger.warning(f"ä»packageæµæå–ZIPæ•°æ®ï¼Œæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤ä¿å­˜ä¸º.xlsx: {extracted_path}")
                                            ole.close()
                                            return '.xlsx'
                                    elif len(package_data) >= 8 and package_data[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':
                                        # OLE2æ ¼å¼ï¼Œå¯èƒ½æ˜¯.xlsæ–‡ä»¶ï¼Œå°è¯•æå–Workbookæµ
                                        try:
                                            import io
                                            package_ole = olefile.OleFileIO(io.BytesIO(package_data))
                                            package_streams = package_ole.listdir()
                                            workbook_found = False
                                            
                                            for pkg_stream in package_streams:
                                                if isinstance(pkg_stream, tuple):
                                                    pkg_stream_first = pkg_stream[0] if len(pkg_stream) > 0 else ''
                                                else:
                                                    pkg_stream_first = str(pkg_stream)
                                                
                                                if pkg_stream_first.lower() in ['workbook', 'book']:
                                                    workbook_data = package_ole.openstream(pkg_stream).read()
                                                    extracted_path = os.path.join(save_dir, f"{ole_id}.xls")
                                                    with open(extracted_path, 'wb') as f:
                                                        f.write(workbook_data)
                                                    logger.info(f"ä»packageæµçš„OLE2æ ¼å¼ä¸­æå–Workbookæµ: {extracted_path}")
                                                    package_ole.close()
                                                    ole.close()
                                                    workbook_found = True
                                                    return '.xls'
                                            
                                            if not workbook_found:
                                                # å¦‚æœæ‰¾ä¸åˆ°Workbookæµï¼Œç›´æ¥ä¿å­˜packageæ•°æ®ä¸º.xls
                                                extracted_path = os.path.join(save_dir, f"{ole_id}.xls")
                                                with open(extracted_path, 'wb') as f:
                                                    f.write(package_data)
                                                logger.info(f"ä»packageæµæå–OLE2æ•°æ®ï¼Œä¿å­˜ä¸º.xls: {extracted_path}")
                                                package_ole.close()
                                                ole.close()
                                                return '.xls'
                                        except Exception as e:
                                            logger.warning(f"ä»packageæµçš„OLE2æ ¼å¼æå–Workbookå¤±è´¥: {e}")
                                            # å¦‚æœæå–å¤±è´¥ï¼Œç›´æ¥ä¿å­˜packageæ•°æ®
                                            extracted_path = os.path.join(save_dir, f"{ole_id}.xls")
                                            with open(extracted_path, 'wb') as f:
                                                f.write(package_data)
                                            logger.info(f"ä»packageæµæå–æ•°æ®ï¼Œä¿å­˜ä¸º.xls: {extracted_path}")
                                            ole.close()
                                            return '.xls'
                                    else:
                                        # å…¶ä»–æ ¼å¼ï¼Œæ ¹æ®ProgIdåˆ¤æ–­
                                        if prog_id and 'excel' in prog_id.lower():
                                            extracted_path = os.path.join(save_dir, f"{ole_id}.xls")
                                            with open(extracted_path, 'wb') as f:
                                                f.write(package_data)
                                            logger.info(f"ä»packageæµæå–æ•°æ®ï¼Œæ ¹æ®ProgIdåˆ¤æ–­ä¸º.xls: {extracted_path}")
                                            ole.close()
                                            return '.xls'
                                        else:
                                            # å¦‚æœæ— æ³•è¯†åˆ«æ ¼å¼ï¼Œä½†ProgIdæ˜¾ç¤ºæ˜¯Excelï¼Œå°è¯•ç›´æ¥ä¿å­˜ä¸º.xls
                                            if prog_id and 'excel' in prog_id.lower():
                                                extracted_path = os.path.join(save_dir, f"{ole_id}.xls")
                                                with open(extracted_path, 'wb') as f:
                                                    f.write(package_data)
                                                logger.info(f"ä»packageæµæå–æ•°æ®ï¼Œæ ¹æ®ProgIdåˆ¤æ–­ä¸º.xls: {extracted_path}")
                                                ole.close()
                                                return '.xls'
                                            else:
                                                logger.warning(f"æ— æ³•è¯†åˆ«packageæµä¸­çš„æ•°æ®æ ¼å¼ï¼ŒProgId: {prog_id}")
                                except Exception as e:
                                    logger.warning(f"ä»packageæµæå–å†…å®¹å¤±è´¥: {e}", exc_info=True)
                        
                        # å¦‚æœæ— æ³•æå–ï¼Œä½†èƒ½åˆ¤æ–­ç±»å‹ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                        # æ³¨æ„ï¼šåªæœ‰åœ¨æ²¡æœ‰æ‰¾åˆ°packageæµæˆ–packageæµæå–å¤±è´¥æ—¶æ‰æ‰§è¡Œ
                        if detected_type and not stream_name_used:
                            logger.warning(f"âœ— æ£€æµ‹åˆ°OLE2æ ¼å¼çš„{detected_type}æ–‡ä»¶ï¼Œä½†æ— æ³•æå–æ ‡å‡†æµï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æµ...")
                            
                            # å°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„Excelæµåï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
                            possible_streams = []
                            for stream_name in stream_list:
                                if isinstance(stream_name, tuple) and len(stream_name) > 0:
                                    stream_name_str = stream_name[0].lower()
                                else:
                                    stream_name_str = str(stream_name).lower()
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«excelç›¸å…³çš„å…³é”®è¯
                                if any(keyword in stream_name_str for keyword in ['workbook', 'book', 'excel', 'sheet', 'xls']):
                                    possible_streams.append(stream_name)
                            
                            if possible_streams:
                                # å°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯èƒ½çš„æµ
                                try:
                                    stream_to_try = possible_streams[0]
                                    extracted_data = ole.openstream(stream_to_try).read()
                                    extracted_path = os.path.join(save_dir, f"{ole_id}{detected_type}")
                                    with open(extracted_path, 'wb') as f:
                                        f.write(extracted_data)
                                    logger.info(f"âœ“ ä½¿ç”¨å¤‡ç”¨æµæå–{detected_type}å†…å®¹æˆåŠŸ: {extracted_path}, æµå: {stream_to_try}")
                                    ole.close()
                                    return detected_type
                                except Exception as e:
                                    logger.warning(f"âœ— ä½¿ç”¨å¤‡ç”¨æµæå–å¤±è´¥: {e}")
                            
                            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›.bin
                            logger.warning(f"âœ— æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°†ä¿å­˜ä¸º.binæ ¼å¼: {ole_id}, å¯ç”¨æµ: {all_streams_debug}")
                            ole.close()
                            return '.bin'  # è¿”å›.binï¼Œè¡¨ç¤ºæ— æ³•æå–ä¸ºæ ‡å‡†æ ¼å¼
                        
                        # å¦‚æœæ—¢æ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æµï¼Œä¹Ÿæ²¡æœ‰æ‰¾åˆ°packageæµï¼Œä½†ProgIdæ˜¾ç¤ºæ˜¯Excelï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰æµ
                        if not stream_name_used and prog_id and 'excel' in prog_id.lower():
                            logger.warning(f"âœ— æ ¹æ®ProgIdåˆ¤æ–­ä¸ºExcelï¼Œä½†æ— æ³•æå–æµï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æµ...")
                            
                            # å°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„Excelæµ
                            possible_streams = []
                            for stream_name in stream_list:
                                if isinstance(stream_name, tuple) and len(stream_name) > 0:
                                    stream_name_str = stream_name[0].lower()
                                else:
                                    stream_name_str = str(stream_name).lower()
                                
                                if any(keyword in stream_name_str for keyword in ['workbook', 'book', 'excel', 'sheet', 'xls']):
                                    possible_streams.append(stream_name)
                            
                            if possible_streams:
                                try:
                                    stream_to_try = possible_streams[0]
                                    extracted_data = ole.openstream(stream_to_try).read()
                                    extracted_path = os.path.join(save_dir, f"{ole_id}.xls")
                                    with open(extracted_path, 'wb') as f:
                                        f.write(extracted_data)
                                    logger.info(f"âœ“ ä½¿ç”¨å¤‡ç”¨æµæå–.xlså†…å®¹æˆåŠŸ: {extracted_path}, æµå: {stream_to_try}")
                                    ole.close()
                                    return '.xls'
                                except Exception as e:
                                    logger.warning(f"âœ— ä½¿ç”¨å¤‡ç”¨æµæå–å¤±è´¥: {e}")
                            
                            logger.warning(f"âœ— æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°†ä¿å­˜ä¸º.binæ ¼å¼: {ole_id}, å¯ç”¨æµ: {all_streams_debug}")
                            ole.close()
                            return '.bin'  # è¿”å›.binï¼Œè¡¨ç¤ºæ— æ³•æå–ä¸ºæ ‡å‡†æ ¼å¼
                        
                    except Exception as e:
                        logger.warning(f"ä»OLE2æ ¼å¼æå–å†…å®¹å¤±è´¥: {e}", exc_info=True)
                    finally:
                        ole.close()
            except ImportError:
                logger.warning("olefileåº“æœªå®‰è£…ï¼Œæ— æ³•æå–OLE2æ ¼å¼ä¸­çš„å†…å®¹ã€‚è¯·å®‰è£…: pip install olefile")
            except Exception as e:
                logger.debug(f"æ£€æµ‹OLE2æ ¼å¼å¤±è´¥: {e}")
            
            # å¦‚æœæ— æ³•æå–ï¼Œä½†èƒ½æ ¹æ®ProgIdåˆ¤æ–­ç±»å‹ï¼Œè¿”å›å¯¹åº”æ‰©å±•å
            if prog_id:
                prog_id_lower = prog_id.lower()
                if 'excel' in prog_id_lower:
                    logger.info(f"æ ¹æ®ProgIdåˆ¤æ–­ä¸ºExcelæ ¼å¼: {prog_id}")
                    return '.xls'
                elif 'word' in prog_id_lower:
                    logger.info(f"æ ¹æ®ProgIdåˆ¤æ–­ä¸ºWordæ ¼å¼: {prog_id}")
                    return '.doc'
                elif 'powerpoint' in prog_id_lower or 'ppt' in prog_id_lower:
                    logger.info(f"æ ¹æ®ProgIdåˆ¤æ–­ä¸ºPowerPointæ ¼å¼: {prog_id}")
                    return '.ppt'
            
            # å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œè¿”å›.bin
            logger.warning(f"æ— æ³•è¯†åˆ«OLE2æ ¼å¼çš„æ–‡ä»¶ç±»å‹ï¼Œè¿”å›.binæ ¼å¼: {ole_id}")
            return '.bin'
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§ç‰ˆExcelæ ¼å¼ï¼ˆ.xlsï¼‰
        # æ—§ç‰ˆExcelä¹Ÿæ˜¯OLE2æ ¼å¼ï¼Œä½†å¯èƒ½æ²¡æœ‰æ ‡å‡†çš„OLE2ç­¾å
        if b'Microsoft Excel' in file_content[:1024] or b'Workbook' in file_content[:1024]:
            return '.xls'
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯Wordæ ¼å¼
        if b'Microsoft Word' in file_content[:1024] or b'WordDocument' in file_content[:1024]:
            return '.doc'
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯PowerPointæ ¼å¼
        if b'Microsoft PowerPoint' in file_content[:1024] or b'PowerPoint' in file_content[:1024]:
            return '.ppt'
        
        # é»˜è®¤è¿”å›åŸå§‹æ‰©å±•åæˆ–.bin
        original_ext = os.path.splitext(original_path)[1].lower()
        if original_ext in ['.xlsx', '.xls', '.docx', '.doc', '.pptx', '.ppt']:
            return original_ext
        
        # å¦‚æœåŸå§‹è·¯å¾„æ²¡æœ‰æ‰©å±•åï¼Œä½†ProgIdæœ‰ä¿¡æ¯ï¼Œå°è¯•æ ¹æ®ProgIdåˆ¤æ–­
        if not original_ext or original_ext == '':
            if prog_id:
                prog_id_lower = prog_id.lower()
                if 'excel' in prog_id_lower:
                    return '.xls'
                elif 'word' in prog_id_lower:
                    return '.doc'
                elif 'powerpoint' in prog_id_lower or 'ppt' in prog_id_lower:
                    return '.ppt'
        
        return '.bin'
    
    @staticmethod
    def _generate_image_description(
        para_text: str,
        prev_paras_text: List[str],
        next_paras_text: List[str],
        section_title: str
    ) -> str:
        """
        ç”Ÿæˆå›¾ç‰‡æè¿°ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        ç­–ç•¥ï¼š
        1. å¦‚æœæ®µè½åŒ…å«å›¾ç‰‡ç›¸å…³å…³é”®è¯ï¼Œä½¿ç”¨æ®µè½æ–‡æœ¬
        2. å¦‚æœå‰ä¸€æ®µè½åŒ…å«å›¾ç‰‡ç›¸å…³å…³é”®è¯ï¼Œä½¿ç”¨å‰ä¸€æ®µè½æ–‡æœ¬
        3. å¦‚æœç« èŠ‚æ ‡é¢˜åŒ…å«å›¾ç‰‡ç›¸å…³å…³é”®è¯ï¼Œä½¿ç”¨ç« èŠ‚æ ‡é¢˜
        4. å¦åˆ™ï¼Œä½¿ç”¨æ®µè½æ–‡æœ¬çš„å‰50å­—ç¬¦
        """
        # å›¾ç‰‡ç›¸å…³å…³é”®è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        image_keywords = [
            'å›¾', 'æµç¨‹å›¾', 'ç¤ºæ„å›¾', 'å›¾ç‰‡', 'å›¾è¡¨', 'æ¶æ„å›¾', 'æ—¶åºå›¾', 
            'ç”¨ä¾‹å›¾', 'ç±»å›¾', 'çŠ¶æ€å›¾', 'æ´»åŠ¨å›¾', 'éƒ¨ç½²å›¾', 'ç»„ä»¶å›¾',
            'figure', 'image', 'diagram', 'chart', 'flowchart'
        ]
        
        # ç­–ç•¥1ï¼šæ£€æŸ¥å½“å‰æ®µè½
        if para_text:
            para_lower = para_text.lower()
            if any(keyword in para_lower for keyword in image_keywords):
                # æå–åŒ…å«å…³é”®è¯çš„å¥å­
                sentences = para_text.split('ã€‚') + para_text.split('.')
                for sentence in sentences:
                    sentence_lower = sentence.lower()
                    if any(keyword in sentence_lower for keyword in image_keywords):
                        return sentence.strip()[:100]
                return para_text[:100]
        
        # ç­–ç•¥2ï¼šæ£€æŸ¥å‰ä¸€æ®µè½
        if prev_paras_text:
            for prev_text in reversed(prev_paras_text):
                prev_lower = prev_text.lower()
                if any(keyword in prev_lower for keyword in image_keywords):
                    return f"ä½äºæ®µè½ï¼š{prev_text[:80]}"
        
        # ç­–ç•¥3ï¼šæ£€æŸ¥ç« èŠ‚æ ‡é¢˜
        if section_title:
            section_lower = section_title.lower()
            if any(keyword in section_lower for keyword in image_keywords):
                return f"{section_title}ä¸­çš„å›¾ç‰‡"
        
        # ç­–ç•¥4ï¼šä½¿ç”¨å½“å‰æ®µè½æ–‡æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if para_text:
            return f"ä½äºæ®µè½ï¼š{para_text[:50]}"
        
        # ç­–ç•¥5ï¼šä½¿ç”¨å‰ä¸€æ®µè½æ–‡æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if prev_paras_text:
            return f"ä½äºæ®µè½ï¼š{prev_paras_text[-1][:50]}"
        
        # é»˜è®¤æè¿°
        return "æ–‡æ¡£ä¸­çš„å›¾ç‰‡"
    
    @staticmethod
    def _format_table_as_text(table_data: Dict) -> str:
        """å°†è¡¨æ ¼æ•°æ®æ ¼å¼åŒ–ä¸ºæ–‡æœ¬æè¿°ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰"""
        if not table_data.get("headers"):
            return ""
        
        # æ„å»ºè¡¨æ ¼æ–‡æœ¬
        text = "è¡¨æ ¼ï¼š\n"
        text += " | ".join(table_data["headers"]) + "\n"
        text += "-" * (len(" | ".join(table_data["headers"]))) + "\n"
        
        for row in table_data.get("rows", []):
            text += " | ".join(row) + "\n"
        
        return text
    
    @staticmethod
    def _format_table_as_markdown(table_data: Dict) -> str:
        """
        å°†è¡¨æ ¼æ•°æ®æ ¼å¼åŒ–ä¸ºæ ‡å‡†Markdownè¡¨æ ¼
        
        Args:
            table_data: {
                "headers": ["åˆ—1", "åˆ—2", "åˆ—3"],
                "rows": [["å€¼1", "å€¼2", "å€¼3"], ...],
                "row_count": int,
                "col_count": int
            }
        
        Returns:
            æ ‡å‡†Markdownæ ¼å¼çš„è¡¨æ ¼å­—ç¬¦ä¸²
        """
        if not table_data.get("headers"):
            return ""
        
        headers = table_data["headers"]
        rows = table_data.get("rows", [])
        
        # æ„å»ºæ ‡å‡†Markdownè¡¨æ ¼
        # è¡¨å¤´è¡Œ
        markdown = "| " + " | ".join(str(header) for header in headers) + " |\n"
        # åˆ†éš”è¡Œï¼ˆæ ‡å‡†Markdownè¡¨æ ¼æ ¼å¼ï¼‰
        markdown += "| " + " | ".join(["---"] * len(headers)) + " |\n"
        
        # æ•°æ®è¡Œ
        for row in rows:
            # ç¡®ä¿è¡Œæ•°æ®é•¿åº¦ä¸è¡¨å¤´ä¸€è‡´
            row_data = row[:len(headers)] if row else []
            if len(row_data) < len(headers):
                row_data.extend([""] * (len(headers) - len(row_data)))
            # è½¬ä¹‰è¡¨æ ¼ä¸­çš„ç®¡é“ç¬¦ï¼Œé¿å…ç ´åè¡¨æ ¼ç»“æ„
            escaped_row = [str(cell).replace("|", "\\|") for cell in row_data]
            markdown += "| " + " | ".join(escaped_row) + " |\n"
        
        return markdown
    
    @staticmethod
    def _extract_images_from_document(doc: Document, document_id: str = None, file_path: str = None) -> List[Dict]:
        """
        æå–æ–‡æ¡£ä¸­çš„å›¾ç‰‡å¹¶ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
        
        å¢å¼ºåŠŸèƒ½ï¼š
        1. æ›´å‡†ç¡®çš„å…³ç³»IDæ˜ å°„ï¼ˆæ”¯æŒå¤šç§è·¯å¾„æ ¼å¼ï¼‰
        2. å›¾ç‰‡å…ƒæ•°æ®æå–ï¼ˆå¤§å°ã€æ ¼å¼ç­‰ï¼‰
        3. åŒ¹é…ç»Ÿè®¡ä¿¡æ¯
        4. å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¤šæ®µè½ã€ç« èŠ‚æ ‡é¢˜ã€ç›¸å¯¹ä½ç½®ï¼‰
        """
        images = []
        match_stats = {
            "total_images": 0,
            "with_rel_id": 0,
            "without_rel_id": 0,
            "matched_by_rel_id": 0,
            "matched_by_drawing": 0,
            "matched_by_keyword": 0,
            "unmatched": 0
        }
        try:
            from docx.oxml.ns import qn
            import zipfile
            import shutil
            
            image_counter = 0
            
            # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
            if document_id and file_path:
                # ä½¿ç”¨document_idåˆ›å»ºå­ç›®å½•
                image_dir = os.path.abspath(f"uploads/extracted_images/{document_id}")
                os.makedirs(image_dir, exist_ok=True)
            else:
                image_dir = os.path.abspath("uploads/extracted_images/temp")
                os.makedirs(image_dir, exist_ok=True)
            
            # ä»docxæ–‡ä»¶ä¸­æå–å›¾ç‰‡ï¼ˆdocxæ˜¯zipæ ¼å¼ï¼‰
            # å»ºç«‹å…³ç³»IDåˆ°å›¾ç‰‡æ–‡ä»¶çš„æ˜ å°„
            rel_id_to_image_file = {}  # {rId: image_file_path}
            image_file_to_rel_id = {}  # {image_file_path: rId}
            
            if file_path and os.path.exists(file_path):
                try:
                    # é¦–å…ˆè¯»å–å…³ç³»æ–‡ä»¶ï¼Œå»ºç«‹å…³ç³»IDåˆ°å›¾ç‰‡æ–‡ä»¶çš„æ˜ å°„
                    with zipfile.ZipFile(file_path, 'r') as zip_file:
                        # è¯»å–document.xml.relsæ–‡ä»¶ï¼Œå»ºç«‹å…³ç³»IDåˆ°å›¾ç‰‡æ–‡ä»¶çš„æ˜ å°„
                        try:
                            rels_file = zip_file.read('word/_rels/document.xml.rels')
                            import xml.etree.ElementTree as ET
                            rels_root = ET.fromstring(rels_file)
                            
                            # è§£æå…³ç³»æ–‡ä»¶ï¼Œæ‰¾åˆ°æ‰€æœ‰å›¾ç‰‡å…³ç³»
                            for rel in rels_root.findall('.//{http://schemas.openxmlformats.org/package/2006/relationships}Relationship'):
                                rel_type = rel.get('Type', '')
                                target = rel.get('Target', '')
                                rel_id = rel.get('Id', '')
                                
                                # å¦‚æœæ˜¯å›¾ç‰‡å…³ç³»
                                if 'image' in rel_type.lower() or target.startswith('media/'):
                                    # æ ‡å‡†åŒ–è·¯å¾„
                                    if target.startswith('media/'):
                                        image_file_path = f"word/{target}"
                                    else:
                                        image_file_path = f"word/media/{target}"
                                    
                                    rel_id_to_image_file[rel_id] = image_file_path
                                    image_file_to_rel_id[image_file_path] = rel_id
                                    logger.debug(f"å»ºç«‹å…³ç³»æ˜ å°„: rId={rel_id} -> {image_file_path}")
                        except Exception as e:
                            logger.warning(f"è¯»å–å…³ç³»æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
                        
                        # ç›´æ¥è§£ææ–‡æ¡£ç»“æ„ï¼Œæ‰¾åˆ°æ‰€æœ‰å›¾ç‰‡å‡ºç°çš„ä½ç½®
                        # ä¸éœ€è¦é¢„å…ˆæŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œè€Œæ˜¯éå†æ–‡æ¡£æ—¶é‡åˆ°å›¾ç‰‡å°±æå–
                        
                        logger.info(f"ğŸ“‹ å»ºç«‹å…³ç³»æ˜ å°„: {len(rel_id_to_image_file)} ä¸ªå›¾ç‰‡å…³ç³»")
                        
                        # ç°åœ¨éå†æ–‡æ¡£æ®µè½ï¼ŒæŒ‰ç…§å›¾ç‰‡åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¡ºåºåˆ†é…image_id
                        # æ„å»ºç« èŠ‚æ ‡é¢˜æ˜ å°„ï¼ˆç”¨äºä¸Šä¸‹æ–‡å¢å¼ºï¼‰
                        section_titles = []
                        current_section_title = ""
                        for para_idx, para in enumerate(doc.paragraphs):
                            if para.style.name.startswith('Heading'):
                                current_section_title = para.text.strip()
                                section_titles.append((para_idx, current_section_title))
                        
                        # è®¡ç®—æ–‡æ¡£æ€»æ®µè½æ•°ï¼ˆç”¨äºç›¸å¯¹ä½ç½®è®¡ç®—ï¼‰
                        total_paragraphs = len(doc.paragraphs)
                        
                        # éå†æ‰€æœ‰æ®µè½ï¼Œæ‰¾åˆ°å›¾ç‰‡å‡ºç°çš„ä½ç½®ï¼ˆæŒ‰æ–‡æ¡£é¡ºåºï¼‰
                        for para_idx, paragraph in enumerate(doc.paragraphs):
                            para_text = paragraph.text.strip()
                            
                            # è·å–å‰åå¤šä¸ªæ®µè½çš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡
                            prev_paras_text = []
                            next_paras_text = []
                            
                            for i in range(max(0, para_idx - 2), para_idx):
                                if i < len(doc.paragraphs):
                                    prev_text = doc.paragraphs[i].text.strip()
                                    if prev_text:
                                        prev_paras_text.append(prev_text)
                            
                            for i in range(para_idx + 1, min(para_idx + 3, len(doc.paragraphs))):
                                if i < len(doc.paragraphs):
                                    next_text = doc.paragraphs[i].text.strip()
                                    if next_text:
                                        next_paras_text.append(next_text)
                            
                            # è·å–æœ€è¿‘çš„ç« èŠ‚æ ‡é¢˜
                            nearest_section_title = ""
                            for section_idx, section_title in reversed(section_titles):
                                if section_idx <= para_idx:
                                    nearest_section_title = section_title
                                    break
                            
                            # è®¡ç®—ç›¸å¯¹ä½ç½®
                            relative_position = para_idx / total_paragraphs if total_paragraphs > 0 else 0.0
                            
                            for run in paragraph.runs:
                                # æ£€æŸ¥runä¸­æ˜¯å¦æœ‰å›¾ç‰‡
                                blips = run._element.xpath('.//a:blip')
                                
                                if blips:
                                    # é€šè¿‡å…³ç³»IDç²¾ç¡®åŒ¹é…å›¾ç‰‡
                                    for blip in blips:
                                        embed_id = blip.get(qn('r:embed'))
                                        link_id = blip.get(qn('r:link'))
                                        rel_id = embed_id or link_id
                                        
                                        if rel_id and rel_id in rel_id_to_image_file:
                                            # æ‰¾åˆ°å›¾ç‰‡ï¼ŒæŒ‰æ–‡æ¡£é¡ºåºåˆ†é…image_id
                                            img_file = rel_id_to_image_file[rel_id]
                                            
                                            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                                            if img_file in zip_file.namelist():
                                                image_counter += 1
                                                image_id = f"image_{image_counter}"
                                                
                                                # è·å–æ–‡ä»¶æ‰©å±•å
                                                ext = os.path.splitext(img_file)[1] or '.png'
                                                file_name = os.path.basename(img_file)
                                                
                                                # ç”Ÿæˆæè¿°
                                                description = WordDocumentService._generate_image_description(
                                                    para_text, prev_paras_text, next_paras_text, nearest_section_title
                                                )
                                                
                                                # ä¿å­˜å›¾ç‰‡æ–‡ä»¶
                                                saved_image_path = os.path.join(image_dir, f"{image_id}{ext}")
                                                with zip_file.open(img_file) as source, open(saved_image_path, 'wb') as target:
                                                    shutil.copyfileobj(source, target)
                                                
                                                # è·å–æ–‡ä»¶å¤§å°å’Œæ ¼å¼
                                                file_size = os.path.getsize(saved_image_path) if os.path.exists(saved_image_path) else 0
                                                file_format = ext[1:].upper() if ext else 'UNKNOWN'  # å»æ‰ç‚¹å·ï¼Œè½¬ä¸ºå¤§å†™
                                                
                                                # è·å–ç›¸å¯¹è·¯å¾„
                                                relative_path = f"extracted_images/{document_id}/{image_id}{ext}" if document_id else f"extracted_images/temp/{image_id}{ext}"
                                                
                                                # æ„å»ºå®Œæ•´çš„å›¾ç‰‡æ•°æ®
                                                images.append({
                                                    "image_id": image_id,
                                                    "position": para_idx,
                                                    "description": description,
                                                    "file_path": saved_image_path,
                                                    "relative_path": relative_path,
                                                    "file_name": file_name,
                                                    "rel_id": rel_id,
                                                    "file_size": file_size,  # æ·»åŠ æ–‡ä»¶å¤§å°
                                                    "file_format": file_format,  # æ·»åŠ æ–‡ä»¶æ ¼å¼
                                                    "context": para_text[:300] if para_text else "",
                                                    "prev_context": " | ".join(prev_paras_text[:2])[:200] if prev_paras_text else "",
                                                    "next_context": " | ".join(next_paras_text[:2])[:200] if next_paras_text else "",
                                                    "section_title": nearest_section_title,
                                                    "relative_position": relative_position,
                                                    "match_method": "rel_id",
                                                    "match_confidence": 1.0
                                                })
                                                
                                                match_stats["matched_by_rel_id"] += 1
                                                match_stats["with_rel_id"] += 1
                                                logger.info(f"âœ… å›¾ç‰‡ {image_id} é€šè¿‡å…³ç³»IDåŒ¹é…åˆ°æ®µè½ {para_idx} (rel_id: {rel_id}, æ–‡ä»¶: {file_name}, ç« èŠ‚: {nearest_section_title[:30]})")
                                                
                                                # æ³¨æ„ï¼šä¸åˆ é™¤ï¼Œå…è®¸åŒä¸€å›¾ç‰‡æ–‡ä»¶å¤šæ¬¡å‡ºç°
                                                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªblipï¼ˆä¸€ä¸ªæ®µè½å¯èƒ½æœ‰å¤šå¼ å›¾ç‰‡ï¼‰
                                            else:
                                                logger.warning(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_file} (rel_id: {rel_id})")
                                
                                
                except Exception as e:
                    logger.warning(f"ä»zipæ–‡ä»¶æå–å›¾ç‰‡å¤±è´¥: {e}", exc_info=True)
            
            # æ³¨æ„ï¼šå›¾ç‰‡ç¼–å·å·²ç»åœ¨zip_fileå—å†…æŒ‰æ–‡æ¡£é¡ºåºåˆ†é…å®Œæˆ
            # æ‰€æœ‰å›¾ç‰‡ï¼ˆåŒ…æ‹¬æœªåŒ¹é…çš„ï¼‰éƒ½å·²ç»åœ¨zip_fileå—å†…å¤„ç†å¹¶åˆ†é…äº†image_id
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            match_stats["total_images"] = len(images)
            match_stats["unmatched"] = sum(1 for img in images if img.get("position") == -1)
            
            # è¾“å‡ºåŒ¹é…ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"ğŸ“Š å›¾ç‰‡åŒ¹é…ç»Ÿè®¡: æ€»æ•°={match_stats['total_images']}, "
                       f"æœ‰rel_id={match_stats['with_rel_id']}, "
                       f"æ— rel_id={match_stats['without_rel_id']}, "
                       f"å…³ç³»IDåŒ¹é…={match_stats['matched_by_rel_id']}, "
                       f"drawingåŒ¹é…={match_stats['matched_by_drawing']}, "
                       f"æœªåŒ¹é…={match_stats['unmatched']}")
            
            # å°†åŒ¹é…ç»Ÿè®¡ä¿¡æ¯æ·»åŠ åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡çš„å…ƒæ•°æ®ä¸­ï¼ˆç”¨äºåç»­åˆ†æï¼‰
            if images:
                images[0]["_match_stats"] = match_stats
            
            logger.info(f"ä»æ–‡æ¡£ä¸­æå–åˆ° {len(images)} å¼ å›¾ç‰‡ï¼Œå·²ä¿å­˜åˆ° {image_dir}")
        except Exception as e:
            logger.warning(f"æå–å›¾ç‰‡æ—¶å‡ºé”™: {e}", exc_info=True)
        
        return images
    
    @staticmethod
    def _split_by_sections(structured_content: List[Dict], max_tokens: int = 8000) -> List[Dict]:
        """æŒ‰ç« èŠ‚åˆ†å—"""
        sections = []
        current_section = None
        
        for item in structured_content:
            if item["type"] == "heading":
                level = item.get("level", 1)
                if level == 1:
                    # ä¸€çº§æ ‡é¢˜ï¼šåˆ›å»ºæ–°ç« èŠ‚
                    if current_section and current_section.get("token_count", 0) > 0:
                        sections.append(current_section)
                    
                    # åˆ›å»ºæ–°ç« èŠ‚
                    current_section = {
                        "section_id": f"section_{len(sections)}",
                        "title": item["text"],
                        "level": level,
                        "content": "",  # ä¸€çº§æ ‡é¢˜ä¸é‡å¤æ·»åŠ åˆ°contentä¸­
                        "token_count": 0,
                        "images": [],
                        "links": item.get("links", []),
                        "tables": []
                    }
                else:
                    # å­æ ‡é¢˜ï¼ˆlevel > 1ï¼‰ï¼šä¿ç•™åœ¨çˆ¶ç« èŠ‚ä¸­ï¼Œä½¿ç”¨Markdownæ ¼å¼
                    if current_section is None:
                        # å¦‚æœæ²¡æœ‰ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç« èŠ‚
                        current_section = {
                            "section_id": "section_0",
                            "title": "æ¦‚è¿°",
                            "level": 1,
                            "content": "",
                            "token_count": 0,
                            "images": [],
                            "links": [],
                            "tables": []
                        }
                    
                    # ä½¿ç”¨Markdownæ ‡é¢˜æ ¼å¼ï¼ˆ##, ###, ####ç­‰ï¼‰
                    heading_markdown = "#" * (level + 1)  # level=2 -> ##, level=3 -> ###
                    heading_text = item["text"]
                    heading_content = f"{heading_markdown} {heading_text}\n\n"
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²ï¼ˆè¶…è¿‡æœ€å¤§ token æ•°ï¼‰
                    heading_tokens = WordDocumentService._estimate_tokens(heading_content)
                    if current_section["token_count"] + heading_tokens > max_tokens:
                        # ä¿å­˜å½“å‰ç« èŠ‚
                        sections.append(current_section)
                        
                        # åˆ›å»ºæ–°ç« èŠ‚ï¼ˆå­ç« èŠ‚ï¼‰
                        current_section = {
                            "section_id": f"section_{len(sections)}",
                            "title": current_section["title"] + "ï¼ˆç»­ï¼‰",
                            "level": current_section["level"],
                            "content": current_section["title"] + "\n\n",
                            "token_count": WordDocumentService._estimate_tokens(current_section["title"]),
                            "images": [],
                            "links": [],
                            "tables": []
                        }
                    
                    # æ·»åŠ å­æ ‡é¢˜åˆ°å†…å®¹ä¸­
                    current_section["content"] += heading_content
                    current_section["token_count"] += heading_tokens
                    
                    # å¤„ç†å­æ ‡é¢˜çš„é“¾æ¥
                    if item.get("links"):
                        current_section["links"].extend(item.get("links", []))
            else:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚
                if current_section is None:
                    # å¦‚æœæ²¡æœ‰ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç« èŠ‚
                    current_section = {
                        "section_id": "section_0",
                        "title": "æ¦‚è¿°",
                        "level": 1,
                        "content": "",
                        "token_count": 0,
                        "images": [],
                        "links": [],
                        "tables": []
                    }
                
                # å¤„ç†è¡¨æ ¼ç±»å‹
                if item["type"] == "table":
                    # è¡¨æ ¼ç›´æ¥æ·»åŠ åˆ°å½“å‰ç« èŠ‚
                    current_section["tables"].append(item["data"])
                    # è¡¨æ ¼æ–‡æœ¬ä¹Ÿæ·»åŠ åˆ°å†…å®¹ä¸­
                    table_text = item.get("text", "")
                    if table_text:
                        item_tokens = WordDocumentService._estimate_tokens(table_text)
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
                        if current_section["token_count"] + item_tokens > max_tokens:
                            # ä¿å­˜å½“å‰ç« èŠ‚
                            sections.append(current_section)
                            
                            # åˆ›å»ºæ–°ç« èŠ‚ï¼ˆå­ç« èŠ‚ï¼‰
                            current_section = {
                                "section_id": f"section_{len(sections)}",
                                "title": current_section["title"] + "ï¼ˆç»­ï¼‰",
                                "level": current_section["level"],
                                "content": current_section["title"] + "\n",
                                "token_count": WordDocumentService._estimate_tokens(current_section["title"]),
                                "images": [],
                                "links": [],
                                "tables": []
                            }
                        current_section["content"] += table_text + "\n"
                        current_section["token_count"] += item_tokens
                else:
                    # å¤„ç†æ®µè½ã€å›¾ç‰‡ç­‰å…¶ä»–ç±»å‹
                    item_text = item.get("text", "")
                    item_tokens = WordDocumentService._estimate_tokens(item_text) if item_text else 0
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²ï¼ˆè¶…è¿‡æœ€å¤§ token æ•°ï¼‰
                    if item_tokens > 0 and current_section["token_count"] + item_tokens > max_tokens:
                        # ä¿å­˜å½“å‰ç« èŠ‚
                        sections.append(current_section)
                        
                        # åˆ›å»ºæ–°ç« èŠ‚ï¼ˆå­ç« èŠ‚ï¼‰
                        current_section = {
                            "section_id": f"section_{len(sections)}",
                            "title": current_section["title"] + "ï¼ˆç»­ï¼‰",
                            "level": current_section["level"],
                            "content": current_section["title"] + "\n",  # ä¿ç•™æ ‡é¢˜
                            "token_count": WordDocumentService._estimate_tokens(current_section["title"]),
                            "images": [],
                            "links": [],
                            "tables": []
                        }
                    
                    # æ·»åŠ å†…å®¹ï¼ˆç¡®ä¿æ®µè½ä¹‹é—´æœ‰é€‚å½“çš„ç©ºè¡Œï¼‰
                    if item_text:
                        current_section["content"] += item_text + "\n\n"
                        current_section["token_count"] += item_tokens
                    
                    # å¤„ç†å›¾ç‰‡
                    if item.get("images"):
                        if "images" not in current_section:
                            current_section["images"] = []
                        current_section["images"].extend(item["images"])
        
        # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚
        if current_section and current_section.get("token_count", 0) > 0:
            sections.append(current_section)
        
        return sections
    
    @staticmethod
    def _split_by_sections_with_strategy(
        structured_content: List[Dict], 
        strategy: str = "level_1",
        max_tokens: int = 8000
    ) -> List[Dict]:
        """
        æ ¹æ®ç­–ç•¥è¿›è¡Œåˆ†å—
        
        ç­–ç•¥ï¼š
        - level_1: æŒ‰ä¸€çº§æ ‡é¢˜åˆ†å—ï¼ˆé»˜è®¤ï¼‰
        - level_2: æŒ‰äºŒçº§æ ‡é¢˜åˆ†å—
        - level_3: æŒ‰ä¸‰çº§æ ‡é¢˜åˆ†å—
        - level_4: æŒ‰å››çº§æ ‡é¢˜åˆ†å—
        - level_5: æŒ‰äº”çº§æ ‡é¢˜åˆ†å—
        - fixed_token: æŒ‰å›ºå®štokenæ•°åˆ†å—
        - no_split: ä¸åˆ†å—ï¼ˆæ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªå—ï¼‰
        
        è¿”å›ï¼š
        æ¯ä¸ªå—åŒ…å« start_index, end_index, content å­—æ®µ
        """
        if strategy == "no_split":
            # ä¸åˆ†å—ï¼šæ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªå—
            return WordDocumentService._split_no_split(structured_content, max_tokens)
        elif strategy == "fixed_token":
            # æŒ‰å›ºå®š token æ•°åˆ†å—
            return WordDocumentService._split_by_fixed_tokens(structured_content, max_tokens)
        elif strategy == "level_2":
            # æŒ‰äºŒçº§æ ‡é¢˜åˆ†å—
            return WordDocumentService._split_by_heading_level(structured_content, 2, max_tokens)
        elif strategy == "level_3":
            # æŒ‰ä¸‰çº§æ ‡é¢˜åˆ†å—
            return WordDocumentService._split_by_heading_level(structured_content, 3, max_tokens)
        elif strategy == "level_4":
            # æŒ‰å››çº§æ ‡é¢˜åˆ†å—
            return WordDocumentService._split_by_heading_level(structured_content, 4, max_tokens)
        elif strategy == "level_5":
            # æŒ‰äº”çº§æ ‡é¢˜åˆ†å—
            return WordDocumentService._split_by_heading_level(structured_content, 5, max_tokens)
        else:  # level_1 æˆ–é»˜è®¤
            # æŒ‰ä¸€çº§æ ‡é¢˜åˆ†å—
            return WordDocumentService._split_by_heading_level(structured_content, 1, max_tokens)
    
    @staticmethod
    def _split_no_split(structured_content: List[Dict], max_tokens: int = 8000) -> List[Dict]:
        """ä¸åˆ†å—ï¼šæ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªå—"""
        content = ""
        total_tokens = 0
        images = []
        tables = []
        links = []
        
        for idx, item in enumerate(structured_content):
            item_text = item.get("text", "")
            if item["type"] == "heading":
                level = item.get("level", 1)
                heading_markdown = "#" * level
                content += f"{heading_markdown} {item_text}\n\n"
            elif item["type"] == "table":
                content += item_text + "\n\n"
                tables.append(item.get("data", []))
            else:
                if item_text:
                    content += item_text + "\n\n"
            
            if item.get("images"):
                images.extend(item["images"])
            if item.get("links"):
                links.extend(item["links"])
            
            total_tokens += WordDocumentService._estimate_tokens(item_text)
        
        # ç¡®å®šæ ‡é¢˜
        title = "å…¨æ–‡æ¡£"
        for item in structured_content:
            if item["type"] == "heading" and item.get("level", 1) == 1:
                title = item.get("text", "å…¨æ–‡æ¡£")
                break
        
        return [{
            "section_id": "chunk_1",
            "title": title,
            "level": 1,
            "content": content.strip(),
            "token_count": total_tokens,
            "start_index": 0,
            "end_index": len(structured_content),
            "images": images,
            "tables": tables,
            "links": links
        }]
    
    @staticmethod
    def _split_by_fixed_tokens(structured_content: List[Dict], max_tokens: int = 8000) -> List[Dict]:
        """æŒ‰å›ºå®š token æ•°åˆ†å—"""
        sections = []
        current_section = {
            "section_id": "chunk_1",
            "title": "æ®µè½ 1",
            "level": 1,
            "content": "",
            "token_count": 0,
            "start_index": 0,
            "end_index": 0,
            "images": [],
            "tables": [],
            "links": []
        }
        
        for idx, item in enumerate(structured_content):
            item_text = item.get("text", "")
            item_tokens = WordDocumentService._estimate_tokens(item_text) if item_text else 0
            
            # æ„å»ºå†…å®¹
            item_content = ""
            if item["type"] == "heading":
                level = item.get("level", 1)
                heading_markdown = "#" * level
                item_content = f"{heading_markdown} {item_text}\n\n"
            elif item["type"] == "table":
                item_content = item_text + "\n\n"
            else:
                if item_text:
                    item_content = item_text + "\n\n"
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—
            if current_section["token_count"] + item_tokens > max_tokens and current_section["token_count"] > 0:
                # ä¿å­˜å½“å‰å—
                current_section["end_index"] = idx
                sections.append(current_section)
                
                # åˆ›å»ºæ–°å—
                chunk_num = len(sections) + 1
                current_section = {
                    "section_id": f"chunk_{chunk_num}",
                    "title": f"æ®µè½ {chunk_num}",
                    "level": 1,
                    "content": "",
                    "token_count": 0,
                    "start_index": idx,
                    "end_index": 0,
                    "images": [],
                    "tables": [],
                    "links": []
                }
            
            # æ·»åŠ å†…å®¹
            current_section["content"] += item_content
            current_section["token_count"] += item_tokens
            
            if item.get("images"):
                current_section["images"].extend(item["images"])
            if item.get("links"):
                current_section["links"].extend(item["links"])
            if item["type"] == "table":
                current_section["tables"].append(item.get("data", []))
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_section["token_count"] > 0:
            current_section["end_index"] = len(structured_content)
            sections.append(current_section)
        
        return sections
    
    @staticmethod
    def _split_by_heading_level(
        structured_content: List[Dict], 
        split_level: int = 1,
        max_tokens: int = 8000
    ) -> List[Dict]:
        """æŒ‰æŒ‡å®šæ ‡é¢˜çº§åˆ«åˆ†å—"""
        sections = []
        current_section = None
        current_start_index = 0
        
        for idx, item in enumerate(structured_content):
            if item["type"] == "heading" and item.get("level", 1) <= split_level:
                # é‡åˆ°åˆ†å‰²æ ‡é¢˜ï¼Œä¿å­˜ä¹‹å‰çš„å—
                if current_section and current_section.get("token_count", 0) > 0:
                    current_section["end_index"] = idx
                    sections.append(current_section)
                
                # åˆ›å»ºæ–°å—
                current_start_index = idx
                current_section = {
                    "section_id": f"chunk_{len(sections) + 1}",
                    "title": item["text"],
                    "level": item.get("level", 1),
                    "content": "",
                    "token_count": 0,
                    "start_index": idx,
                    "end_index": 0,
                    "images": [],
                    "tables": [],
                    "links": item.get("links", [])
                }
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                if current_section is None:
                    # å¦‚æœæ²¡æœ‰å—ï¼Œåˆ›å»ºé»˜è®¤å—
                    current_section = {
                        "section_id": "chunk_1",
                        "title": "æ¦‚è¿°",
                        "level": 1,
                        "content": "",
                        "token_count": 0,
                        "start_index": 0,
                        "end_index": 0,
                        "images": [],
                        "tables": [],
                        "links": []
                    }
                
                item_text = item.get("text", "")
                item_tokens = WordDocumentService._estimate_tokens(item_text) if item_text else 0
                
                # æ„å»ºå†…å®¹
                if item["type"] == "heading":
                    level = item.get("level", 1)
                    heading_markdown = "#" * level
                    current_section["content"] += f"{heading_markdown} {item_text}\n\n"
                elif item["type"] == "table":
                    current_section["content"] += item_text + "\n\n"
                    current_section["tables"].append(item.get("data", []))
                else:
                    if item_text:
                        current_section["content"] += item_text + "\n\n"
                
                current_section["token_count"] += item_tokens
                
                if item.get("images"):
                    current_section["images"].extend(item["images"])
                if item.get("links"):
                    current_section["links"].extend(item["links"])
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_section and current_section.get("token_count", 0) > 0:
            current_section["end_index"] = len(structured_content)
            sections.append(current_section)
        
        return sections
    
    @staticmethod
    def _extract_formatted_text(paragraph) -> str:
        """
        æå–æ®µè½æ–‡æœ¬ï¼Œä¿ç•™æ ¼å¼ä¿¡æ¯ï¼ˆè½¬æ¢ä¸ºMarkdownï¼‰
        
        Args:
            paragraph: docxæ®µè½å¯¹è±¡
        
        Returns:
            æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        formatted_text = ""
        for run in paragraph.runs:
            text = run.text
            if not text:
                continue
            
            # å¤„ç†åŠ ç²—
            if run.bold:
                text = f"**{text}**"
            
            # å¤„ç†æ–œä½“
            if run.italic:
                text = f"*{text}*"
            
            # å¤„ç†ä¸‹åˆ’çº¿ï¼ˆMarkdownä¸ç›´æ¥æ”¯æŒï¼Œä½¿ç”¨HTMLï¼‰
            if run.underline:
                text = f"<u>{text}</u>"
            
            # å¤„ç†åˆ é™¤çº¿
            if hasattr(run, 'strike') and run.strike:
                text = f"~~{text}~~"
            
            formatted_text += text
        
        return formatted_text.strip()
    
    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ token æ•°ï¼ˆä¸­æ–‡é€šå¸¸ 1 token â‰ˆ 2 å­—ç¬¦ï¼‰"""
        return len(text) // 2
    
    @staticmethod
    def _extract_base_name(document_name: str) -> str:
        """
        ä»æ–‡æ¡£åç§°ä¸­æå–åŸºç¡€æ ‡è¯†ï¼ˆå»é™¤ç‰ˆæœ¬å·ï¼‰
        
        Args:
            document_name: æ–‡æ¡£åç§°ï¼Œä¾‹å¦‚ "äº§ä¸šé¡¹ç›®-é¡¹ç›®é‡Œç¨‹ç¢‘ç®¡ç†-è½¯ä»¶éœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦-20230731- V1"
        
        Returns:
            åŸºç¡€æ ‡è¯†ï¼Œä¾‹å¦‚ "äº§ä¸šé¡¹ç›®-é¡¹ç›®é‡Œç¨‹ç¢‘ç®¡ç†-è½¯ä»¶éœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦-20230731"
        """
        import re
        
        # æ”¯æŒå¤šç§ç‰ˆæœ¬å·æ ¼å¼
        patterns = [
            r'\s*-\s*V\d+$',           # " - V1"
            r'\s*-\s*v\d+$',           # " - v1"
            r'\s*ç‰ˆæœ¬\d+$',             # " ç‰ˆæœ¬1"
            r'\s*Version\s*\d+$',      # " Version 1"
            r'\s*version\s*\d+$',      # " version 1"
        ]
        
        base_name = document_name
        for pattern in patterns:
            base_name = re.sub(pattern, '', base_name, flags=re.IGNORECASE)
        
        return base_name.strip()
    
    @staticmethod
    def _extract_version(document_name: str) -> tuple[str, int]:
        """
        ä»æ–‡æ¡£åç§°ä¸­æå–ç‰ˆæœ¬å·
        
        Args:
            document_name: æ–‡æ¡£åç§°
        
        Returns:
            (version_string, version_number) ä¾‹å¦‚ ("V1", 1)
        """
        import re
        
        version_match = re.search(r'V(\d+)', document_name, re.IGNORECASE)
        if version_match:
            version_num = int(version_match.group(1))
            return f"V{version_num}", version_num
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›é»˜è®¤å€¼
        return "V1", 1
    
    @staticmethod
    def _sanitize_group_id(name: str) -> str:
        """
        æ¸…ç† group_idï¼Œåªä¿ç•™å­—æ¯æ•°å­—ã€ç ´æŠ˜å·å’Œä¸‹åˆ’çº¿
        
        Graphiti è¦æ±‚ group_id åªèƒ½åŒ…å« alphanumeric characters, dashes, or underscores
        
        Args:
            name: åŸå§‹åç§°ï¼ˆå¯èƒ½åŒ…å«ä¸­æ–‡ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰
        
        Returns:
            æ¸…ç†åçš„åç§°ï¼ˆåªåŒ…å«å­—æ¯æ•°å­—ã€ç ´æŠ˜å·ã€ä¸‹åˆ’çº¿ï¼‰
        """
        import re
        
        # å°†ä¸­æ–‡å­—ç¬¦å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        # åªä¿ç•™å­—æ¯æ•°å­—ã€ç ´æŠ˜å·ã€ä¸‹åˆ’çº¿
        sanitized = re.sub(r'[^a-zA-Z0-9\-_]', '_', name)
        
        # å°†è¿ç»­çš„ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºå•ä¸ªä¸‹åˆ’çº¿
        sanitized = re.sub(r'_+', '_', sanitized)
        
        # å»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
        sanitized = sanitized.strip('_')
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not sanitized:
            sanitized = "document"
        
        # é™åˆ¶é•¿åº¦ï¼ˆé¿å…è¿‡é•¿ï¼‰
        if len(sanitized) > 50:
            sanitized = sanitized[:50]
        
        return sanitized
    
    @staticmethod
    def _build_section_content(section: Dict, doc_data: Dict, section_idx: int, document_id: str = None, upload_id: int = None) -> str:
        """
        æ„å»ºç« èŠ‚å†…å®¹ï¼ˆ1:1å¯¹åº”åŸå§‹æ–‡æ¡£ï¼Œä¸æ·»åŠ é¢å¤–æè¿°ï¼‰
        
        åªåŒ…å«åŸå§‹æ–‡æ¡£ä¸­çš„å†…å®¹ï¼š
        - æ ‡é¢˜å’Œæ®µè½æ–‡å­—
        - å›¾ç‰‡ï¼š![alt](url)ï¼ˆaltä½¿ç”¨åŸå§‹å›¾æ³¨æˆ–ç©ºï¼‰
        - è¡¨æ ¼ï¼šåªä¿ç•™è¡¨æ ¼å†…å®¹ï¼Œä¸æ·»åŠ æ ‡é¢˜ï¼ˆé™¤éåŸå§‹æ–‡æ¡£æœ‰ï¼‰
        - åµŒå…¥æ–‡æ¡£ï¼šç®€å•å ä½ç¬¦
        - é“¾æ¥ï¼šä¿ç•™åœ¨åŸå§‹ä½ç½®
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç›´æ¥éå†æ•´ä¸ªstructured_contentï¼ŒæŒ‰åŸå§‹é¡ºåºè¾“å‡ºï¼Œç¡®ä¿1:1å¯¹åº”ã€‚
        å¦‚æœæ–‡æ¡£æœ‰å¤šä¸ªç« èŠ‚ï¼Œæ¯ä¸ªç« èŠ‚ä¼šè¾“å‡ºä¸€æ¬¡ï¼Œä½†å†…å®¹ä¸ä¼šé‡å¤ã€‚
        """
        # å°†document_idæ·»åŠ åˆ°doc_dataä¸­ï¼Œä»¥ä¾¿åœ¨æ„å»ºå†…å®¹æ—¶ä½¿ç”¨
        if document_id and 'document_id' not in doc_data:
            doc_data['document_id'] = document_id
        
        # æ‰¾åˆ°å½“å‰ç« èŠ‚åœ¨structured_contentä¸­çš„èŒƒå›´
        section_start_idx = None
        section_end_idx = None
        section_title_found = False  # æ ‡è®°æ˜¯å¦åœ¨åŸå§‹æ–‡æ¡£ä¸­æ‰¾åˆ°è¯¥ç« èŠ‚æ ‡é¢˜
        
        # æ‰¾åˆ°å½“å‰ç« èŠ‚çš„èµ·å§‹ä½ç½®ï¼ˆä¸€çº§æ ‡é¢˜ï¼‰
        for idx, item in enumerate(doc_data.get("structured_content", [])):
            if item.get("type") == "heading" and item.get("level", 1) == 1:
                # å¦‚æœæ‰¾åˆ°äº†åŒ¹é…çš„æ ‡é¢˜ï¼Œè®°å½•èµ·å§‹ä½ç½®
                if item.get("text") == section.get("title"):
                    section_start_idx = idx
                    section_title_found = True
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªä¸€çº§æ ‡é¢˜ä½œä¸ºç»“æŸä½ç½®
                    for next_idx in range(idx + 1, len(doc_data.get("structured_content", []))):
                        next_item = doc_data["structured_content"][next_idx]
                        if next_item.get("type") == "heading" and next_item.get("level", 1) == 1:
                            section_end_idx = next_idx
                            break
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ ‡é¢˜ï¼ˆå¯èƒ½æ˜¯ç³»ç»Ÿç”Ÿæˆçš„"æ¦‚è¿°"ï¼‰ï¼Œä¸è¾“å‡ºä»»ä½•å†…å®¹
        # è¿™æ ·å¯ä»¥é¿å…é‡å¤è¾“å‡ºæ•´ä¸ªæ–‡æ¡£
        if not section_title_found:
            # å¦‚æœæ ‡é¢˜æ˜¯ç³»ç»Ÿç”Ÿæˆçš„"æ¦‚è¿°"ï¼Œä¸”åŸå§‹æ–‡æ¡£ä¸­æ²¡æœ‰ä¸€çº§æ ‡é¢˜ï¼Œåˆ™è¾“å‡ºæ•´ä¸ªæ–‡æ¡£
            # ä½†åªè¾“å‡ºä¸€æ¬¡ï¼ˆç¬¬ä¸€ä¸ªç« èŠ‚ï¼‰
            if section.get("title") == "æ¦‚è¿°" and section_idx == 0:
                # æ£€æŸ¥åŸå§‹æ–‡æ¡£æ˜¯å¦çœŸçš„æ²¡æœ‰ä¸€çº§æ ‡é¢˜
                has_level1_heading = any(
                    item.get("type") == "heading" and item.get("level", 1) == 1
                    for item in doc_data.get("structured_content", [])
                )
                if not has_level1_heading:
                    # åŸå§‹æ–‡æ¡£ç¡®å®æ²¡æœ‰ä¸€çº§æ ‡é¢˜ï¼Œè¾“å‡ºæ•´ä¸ªæ–‡æ¡£
                    section_start_idx = 0
                    section_end_idx = len(doc_data.get("structured_content", []))
                else:
                    # åŸå§‹æ–‡æ¡£æœ‰ä¸€çº§æ ‡é¢˜ï¼Œä½†å½“å‰ç« èŠ‚æ ‡é¢˜ä¸åŒ¹é…ï¼Œä¸è¾“å‡º
                    return ""
            else:
                # å…¶ä»–æƒ…å†µï¼Œä¸è¾“å‡º
                return ""
        
        # å¦‚æœæ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼Œæ„å»ºå†…å®¹
        if section_start_idx is None:
            return ""
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æŸä½ç½®ï¼Œä½¿ç”¨æ–‡æ¡£æœ«å°¾
        if section_end_idx is None:
            section_end_idx = len(doc_data.get("structured_content", []))
        
        # æ„å»ºç« èŠ‚å†…å®¹ï¼ˆæŒ‰ç…§structured_contentçš„é¡ºåºï¼‰
        # åªæœ‰å½“åŸå§‹æ–‡æ¡£ä¸­å­˜åœ¨è¯¥ç« èŠ‚æ ‡é¢˜æ—¶ï¼Œæ‰è¾“å‡ºæ ‡é¢˜
        # æ³¨æ„ï¼šæ ‡é¢˜æ–‡æœ¬åº”è¯¥åŒ…å«åŸå§‹æ–‡æ¡£ä¸­çš„å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…æ‹¬æ•°å­—å‰ç¼€ï¼Œå¦‚"1 é¡¹ç›®é‡Œç¨‹ç¢‘ç®¡ç†"ï¼‰
        content = ""
        if section_title_found:
            # ä»åŸå§‹æ–‡æ¡£ä¸­è·å–å®Œæ•´çš„æ ‡é¢˜æ–‡æœ¬ï¼ˆåŒ…å«æ•°å­—å‰ç¼€ï¼‰
            original_title_text = ""
            for idx, item in enumerate(doc_data.get("structured_content", [])):
                if idx == section_start_idx and item.get("type") == "heading":
                    original_title_text = item.get("text", section.get("title", ""))
                    break
            # å¦‚æœæ‰¾åˆ°äº†åŸå§‹æ ‡é¢˜æ–‡æœ¬ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨sectionä¸­çš„title
            title_to_use = original_title_text if original_title_text else section.get("title", "")
            content = f"# {title_to_use}\n\n"
        
        # æŒ‰ç…§structured_contentçš„é¡ºåºæ„å»ºå†…å®¹
        for idx in range(section_start_idx, min(section_end_idx, len(doc_data.get("structured_content", [])))):
            if idx >= len(doc_data.get("structured_content", [])):
                break
            item = doc_data["structured_content"][idx]
            
            # è·³è¿‡ä¸€çº§æ ‡é¢˜ï¼ˆå·²ç»åœ¨ä¸Šé¢æ·»åŠ äº†ï¼‰
            if item.get("type") == "heading" and item.get("level", 1) == 1:
                continue
            
            # å¤„ç†å­æ ‡é¢˜ï¼ˆlevel > 1ï¼‰
            if item.get("type") == "heading" and item.get("level", 1) > 1:
                level = item.get("level", 2)
                heading_markdown = "#" * (level + 1)  # level=2 -> ##, level=3 -> ###
                heading_text = item.get("text", "")
                content += f"{heading_markdown} {heading_text}\n\n"
            
            # å¤„ç†æ®µè½
            elif item.get("type") == "paragraph":
                paragraph_text = item.get("text", "")
                if paragraph_text:
                    content += f"{paragraph_text}\n\n"
                
                # å¦‚æœæ®µè½æœ‰å…³è”çš„å›¾ç‰‡ï¼Œç«‹å³æ’å…¥ï¼ˆä¿ç•™åŸå§‹ä½ç½®ï¼‰
                # åªä¿ç•™å›¾ç‰‡é“¾æ¥ï¼Œä¸æ·»åŠ é¢å¤–æè¿°
                if item.get("images"):
                    for image in item["images"]:
                        image_id = image.get('image_id', '')
                        relative_path = image.get('relative_path', '')
                        
                        # å°è¯•è·å–åŸå§‹å›¾æ³¨ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ç©ºalt text
                        # æ³¨æ„ï¼šå½“å‰ä»£ç ä¸­æ²¡æœ‰æå–åŸå§‹å›¾æ³¨çš„é€»è¾‘ï¼Œæš‚æ—¶ä½¿ç”¨ç©ºalt text
                        alt_text = ""  # å¯ä»¥åç»­æ‰©å±•æå–åŸå§‹å›¾æ³¨çš„é€»è¾‘
                        
                        if relative_path and doc_data.get('document_id'):
                            document_id = doc_data.get('document_id')
                            image_url = f"/api/word-document/{document_id}/images/{image_id}"
                            content += f"![{alt_text}]({image_url})\n\n"
            
            # å¤„ç†è¡¨æ ¼ï¼ˆä¿ç•™åŸå§‹ä½ç½®ï¼‰
            # ä¸æ·»åŠ ç³»ç»Ÿç”Ÿæˆçš„æ ‡é¢˜ï¼Œåªä¿ç•™è¡¨æ ¼å†…å®¹
            elif item.get("type") == "table":
                table_data = item.get("data", {})
                
                # æ³¨æ„ï¼šå½“å‰ä»£ç ä¸­æ²¡æœ‰æå–åŸå§‹è¡¨æ ¼æ ‡é¢˜çš„é€»è¾‘
                # å¦‚æœåç»­éœ€è¦æ”¯æŒåŸå§‹è¡¨æ ¼æ ‡é¢˜ï¼Œéœ€è¦åœ¨è¿™é‡Œæ·»åŠ æå–é€»è¾‘
                # æš‚æ—¶åªä¿ç•™è¡¨æ ¼å†…å®¹ï¼Œä¸æ·»åŠ ä»»ä½•æ ‡é¢˜
                
                # ä½¿ç”¨æ ‡å‡†Markdownè¡¨æ ¼æ ¼å¼
                content += WordDocumentService._format_table_as_markdown(table_data) + "\n\n"
            
            # å¤„ç†image_onlyç±»å‹ï¼ˆå•ç‹¬çš„å›¾ç‰‡ï¼‰
            elif item.get("type") == "image_only":
                if item.get("images"):
                    for image in item["images"]:
                        image_id = image.get('image_id', '')
                        relative_path = image.get('relative_path', '')
                        
                        # åªä¿ç•™å›¾ç‰‡é“¾æ¥ï¼Œä¸æ·»åŠ é¢å¤–æè¿°
                        alt_text = ""  # å¯ä»¥åç»­æ‰©å±•æå–åŸå§‹å›¾æ³¨çš„é€»è¾‘
                        
                        if relative_path and doc_data.get('document_id'):
                            document_id = doc_data.get('document_id')
                            image_url = f"/api/word-document/{document_id}/images/{image_id}"
                            content += f"![{alt_text}]({image_url})\n\n"
            
            # å¤„ç†OLEå¯¹è±¡ï¼ˆåµŒå…¥æ–‡æ¡£ï¼‰
            # ç”ŸæˆåŒ…å«å®Œæ•´ä¿¡æ¯çš„Markdownæ ¼å¼ï¼Œä¸"éœ€æ±‚ç®¡ç†"çš„æ˜¾ç¤ºæ–¹å¼ä¸€è‡´
            if item.get("ole_objects"):
                for ole in item.get("ole_objects", []):
                    ole_id = ole.get('ole_id', '')
                    ole_name = ole.get('name', 'åµŒå…¥æ–‡æ¡£')
                    ole_type = ole.get('type', 'åµŒå…¥å¯¹è±¡')
                    document_id = doc_data.get('document_id', '')
                    
                    # å¦‚æœupload_idå­˜åœ¨ï¼Œä½¿ç”¨document-upload APIï¼›å¦åˆ™ä½¿ç”¨word-document API
                    if upload_id and ole_id:
                        preview_url = f"/api/document-upload/{upload_id}/ole/{ole_id}?view=preview"
                        download_url = f"/api/document-upload/{upload_id}/ole/{ole_id}?view=download"
                        # ç”ŸæˆåŒ…å«æ–‡ä»¶åã€ç±»å‹ã€æŸ¥çœ‹/ä¸‹è½½é“¾æ¥çš„Markdown
                        content += f"[åµŒå…¥æ–‡æ¡£: {ole_name} ({ole_type})]({preview_url})\n"
                        content += f"[æŸ¥çœ‹]({preview_url}) | [ä¸‹è½½]({download_url})\n\n"
                    elif document_id and ole_id:
                        # å…¼å®¹æ—§æ ¼å¼ï¼ˆä½¿ç”¨word-document APIï¼‰
                        preview_url = f"/api/word-document/{document_id}/ole/{ole_id}?view=preview"
                        download_url = f"/api/word-document/{document_id}/ole/{ole_id}?view=download"
                        content += f"[åµŒå…¥æ–‡æ¡£: {ole_name} ({ole_type})]({preview_url})\n"
                        content += f"[æŸ¥çœ‹]({preview_url}) | [ä¸‹è½½]({download_url})\n\n"
                    else:
                        content += "[åµŒå…¥æ–‡æ¡£]\n\n"
        
        # æ³¨æ„ï¼šä¸å†æ·»åŠ ç« èŠ‚æœ«å°¾çš„é“¾æ¥åˆ—è¡¨ï¼Œé“¾æ¥ä¿ç•™åœ¨åŸå§‹ä½ç½®ï¼ˆæ®µè½ä¸­ï¼‰
        
        return content
    
    @staticmethod
    def _build_content_from_items(items: List[Dict], doc_data: Dict, document_id: str = None, upload_id: int = None) -> str:
        """
        ä»structured_contentçš„itemsåˆ—è¡¨æ„å»ºå†…å®¹ï¼ˆç”¨äºå°é¢é¡µã€è¡¨æ ¼ç­‰ï¼‰
        
        è¿™ä¸ªæ–¹æ³•ç”¨äºæ„å»ºç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜ä¹‹å‰çš„å†…å®¹ï¼Œç¡®ä¿1:1å¯¹åº”åŸå§‹æ–‡æ¡£
        
        Args:
            items: structured_contentçš„å­åˆ—è¡¨
            doc_data: å®Œæ•´çš„æ–‡æ¡£æ•°æ®
            document_id: æ–‡æ¡£IDï¼ˆç”¨äºæ„å»ºå›¾ç‰‡å’ŒOLEå¯¹è±¡çš„URLï¼‰
        """
        if document_id and 'document_id' not in doc_data:
            doc_data['document_id'] = document_id
        
        content = ""
        
        for item in items:
            # å¤„ç†æ®µè½
            if item.get("type") == "paragraph":
                paragraph_text = item.get("text", "")
                if paragraph_text:
                    content += f"{paragraph_text}\n\n"
                
                # å¦‚æœæ®µè½æœ‰å…³è”çš„å›¾ç‰‡ï¼Œç«‹å³æ’å…¥ï¼ˆä¿ç•™åŸå§‹ä½ç½®ï¼‰
                if item.get("images"):
                    for image in item["images"]:
                        image_id = image.get('image_id', '')
                        relative_path = image.get('relative_path', '')
                        alt_text = ""  # å¯ä»¥åç»­æ‰©å±•æå–åŸå§‹å›¾æ³¨çš„é€»è¾‘
                        
                        if relative_path and doc_data.get('document_id'):
                            document_id = doc_data.get('document_id')
                            image_url = f"/api/word-document/{document_id}/images/{image_id}"
                            content += f"![{alt_text}]({image_url})\n\n"
            
            # å¤„ç†æ ‡é¢˜ï¼ˆå¯èƒ½æ˜¯å°é¢é¡µçš„æ ‡é¢˜ï¼Œä¸æ˜¯ä¸€çº§æ ‡é¢˜ï¼‰
            elif item.get("type") == "heading":
                level = item.get("level", 1)
                heading_text = item.get("text", "")
                if heading_text:
                    # ä½¿ç”¨å¯¹åº”çš„Markdownæ ‡é¢˜çº§åˆ«
                    heading_markdown = "#" * (level + 1)  # level=1 -> ##, level=2 -> ###
                    content += f"{heading_markdown} {heading_text}\n\n"
            
            # å¤„ç†è¡¨æ ¼ï¼ˆä¿ç•™åŸå§‹ä½ç½®ï¼‰
            elif item.get("type") == "table":
                table_data = item.get("data", {})
                # å¦‚æœåŸå§‹Wordæ–‡æ¡£æœ‰è¡¨æ ¼æ ‡é¢˜ï¼Œåˆ™ä¿ç•™
                if table_data.get("caption"):
                    content += f"### {table_data['caption']}\n\n"
                # ä½¿ç”¨æ ‡å‡†Markdownè¡¨æ ¼æ ¼å¼
                content += WordDocumentService._format_table_as_markdown(table_data) + "\n\n"
            
            # å¤„ç†image_onlyç±»å‹ï¼ˆå•ç‹¬çš„å›¾ç‰‡ï¼‰
            elif item.get("type") == "image_only":
                if item.get("images"):
                    for image in item["images"]:
                        image_id = image.get('image_id', '')
                        relative_path = image.get('relative_path', '')
                        alt_text = ""
                        
                        if relative_path and doc_data.get('document_id'):
                            document_id = doc_data.get('document_id')
                            image_url = f"/api/word-document/{document_id}/images/{image_id}"
                            content += f"![{alt_text}]({image_url})\n\n"
            
            # å¤„ç†OLEå¯¹è±¡ï¼ˆåµŒå…¥æ–‡æ¡£ï¼‰
            # ç”ŸæˆåŒ…å«å®Œæ•´ä¿¡æ¯çš„Markdownæ ¼å¼ï¼Œä¸"éœ€æ±‚ç®¡ç†"çš„æ˜¾ç¤ºæ–¹å¼ä¸€è‡´
            if item.get("ole_objects"):
                for ole in item.get("ole_objects", []):
                    ole_id = ole.get('ole_id', '')
                    ole_name = ole.get('name', 'åµŒå…¥æ–‡æ¡£')
                    ole_type = ole.get('type', 'åµŒå…¥å¯¹è±¡')
                    document_id = doc_data.get('document_id', '')
                    
                    # å¦‚æœupload_idå­˜åœ¨ï¼Œä½¿ç”¨document-upload APIï¼›å¦åˆ™ä½¿ç”¨word-document API
                    if upload_id and ole_id:
                        preview_url = f"/api/document-upload/{upload_id}/ole/{ole_id}?view=preview"
                        download_url = f"/api/document-upload/{upload_id}/ole/{ole_id}?view=download"
                        # ç”ŸæˆåŒ…å«æ–‡ä»¶åã€ç±»å‹ã€æŸ¥çœ‹/ä¸‹è½½é“¾æ¥çš„Markdown
                        content += f"[åµŒå…¥æ–‡æ¡£: {ole_name} ({ole_type})]({preview_url})\n"
                        content += f"[æŸ¥çœ‹]({preview_url}) | [ä¸‹è½½]({download_url})\n\n"
                    elif document_id and ole_id:
                        # å…¼å®¹æ—§æ ¼å¼ï¼ˆä½¿ç”¨word-document APIï¼‰
                        preview_url = f"/api/word-document/{document_id}/ole/{ole_id}?view=preview"
                        download_url = f"/api/word-document/{document_id}/ole/{ole_id}?view=download"
                        content += f"[åµŒå…¥æ–‡æ¡£: {ole_name} ({ole_type})]({preview_url})\n"
                        content += f"[æŸ¥çœ‹]({preview_url}) | [ä¸‹è½½]({download_url})\n\n"
                    else:
                        content += "[åµŒå…¥æ–‡æ¡£]\n\n"
        
        return content.strip()
    
    @staticmethod
    def _extract_author_from_content(doc_data: Dict) -> Optional[str]:
        """
        ä»æ–‡æ¡£å†…å®¹ä¸­æå–ä½œè€…ä¿¡æ¯
        
        ä¼˜å…ˆä»"æ–‡æ¡£ä¿®æ”¹è®°å½•"è¡¨æ ¼ä¸­æå–ä½œè€…ä¿¡æ¯
        
        Args:
            doc_data: è§£æåçš„æ–‡æ¡£æ•°æ®
            
        Returns:
            ä½œè€…ä¿¡æ¯ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        # æŸ¥æ‰¾"æ–‡æ¡£ä¿®æ”¹è®°å½•"è¡¨æ ¼
        # è¡¨æ ¼æ ‡é¢˜å¯èƒ½åŒ…å«ï¼šæ–‡æ¡£ä¿®æ”¹è®°å½•ã€ä¿®æ”¹è®°å½•ã€ä¿®è®¢è®°å½•ç­‰
        modification_record_keywords = ["æ–‡æ¡£ä¿®æ”¹è®°å½•", "ä¿®æ”¹è®°å½•", "ä¿®è®¢è®°å½•", "ç‰ˆæœ¬è®°å½•"]
        
        for table_data in doc_data.get("tables", []):
            headers = table_data.get("headers", [])
            rows = table_data.get("rows", [])
            
            # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦åŒ…å«"ä½œè€…"åˆ—
            if "ä½œè€…" not in headers:
                continue
            
            # æŸ¥æ‰¾"ä½œè€…"åˆ—çš„ç´¢å¼•
            author_col_idx = None
            for idx, header in enumerate(headers):
                if "ä½œè€…" in header:
                    author_col_idx = idx
                break
            
            if author_col_idx is None:
                continue
            
            # æ£€æŸ¥è¡¨æ ¼å‰çš„æ ‡é¢˜æ˜¯å¦æ˜¯"æ–‡æ¡£ä¿®æ”¹è®°å½•"ç›¸å…³
            # æŸ¥æ‰¾è¡¨æ ¼åœ¨structured_contentä¸­çš„ä½ç½®
            table_id = table_data.get("table_id", "")
            for item in doc_data.get("structured_content", []):
                if item.get("type") == "table" and item.get("table_id") == table_id:
                    # æŸ¥æ‰¾è¡¨æ ¼å‰çš„æ ‡é¢˜
                    item_idx = doc_data.get("structured_content", []).index(item)
                    # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„æ ‡é¢˜
                    for prev_idx in range(item_idx - 1, max(-1, item_idx - 10), -1):
                        prev_item = doc_data.get("structured_content", [])[prev_idx]
                        if prev_item.get("type") == "heading" or prev_item.get("type") == "paragraph":
                            prev_text = prev_item.get("text", "")
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¿®æ”¹è®°å½•ç›¸å…³çš„å…³é”®è¯
                            if any(keyword in prev_text for keyword in modification_record_keywords):
                                # æ‰¾åˆ°äº†"æ–‡æ¡£ä¿®æ”¹è®°å½•"è¡¨æ ¼ï¼Œæå–ä½œè€…ä¿¡æ¯
                                # ä»ç¬¬ä¸€è¡Œæ•°æ®ä¸­æå–ä½œè€…ï¼ˆé€šå¸¸æ˜¯æœ€æ–°çš„ç‰ˆæœ¬ï¼‰
                                if rows and len(rows) > 0:
                                    author = rows[0][author_col_idx].strip() if author_col_idx < len(rows[0]) else ""
                                    if author:
                                        # å»é‡ï¼šå¦‚æœå¤šè¡Œä½œè€…ç›¸åŒï¼Œåªè¿”å›ä¸€ä¸ª
                                        unique_authors = set()
                                        for row in rows:
                                            if author_col_idx < len(row):
                                                author_name = row[author_col_idx].strip()
                                                if author_name:
                                                    unique_authors.add(author_name)
                                        
                                        # å¦‚æœæ‰€æœ‰ä½œè€…éƒ½ç›¸åŒï¼Œè¿”å›ä¸€ä¸ªï¼›å¦åˆ™è¿”å›æ‰€æœ‰ä½œè€…ï¼ˆç”¨é¡¿å·åˆ†éš”ï¼‰
                                        if len(unique_authors) == 1:
                                            return list(unique_authors)[0]
                                        elif len(unique_authors) > 1:
                                            return "ã€".join(sorted(unique_authors))
                                        else:
                                            return author
                                break
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›Noneï¼ˆä¸æ˜¾ç¤ºä½œè€…ï¼‰
        return None
    
    @staticmethod
    def _extract_overview_content(structured_content: List[Dict]) -> Optional[str]:
        """
        ä»ç»“æ„åŒ–å†…å®¹ä¸­æå–åŠŸèƒ½æ¦‚è¿°
        
        æŸ¥æ‰¾ç¬¬ä¸€ä¸ª"æ¦‚è¿°"ç›¸å…³ç« èŠ‚çš„å†…å®¹ï¼Œå¦‚åŠŸèƒ½æ¦‚è¿°ã€ç³»ç»Ÿæ¦‚è¿°ã€é¡¹ç›®æ¦‚è¿°ç­‰
        
        Args:
            structured_content: ç»“æ„åŒ–å†…å®¹åˆ—è¡¨
            
        Returns:
            åŠŸèƒ½æ¦‚è¿°å†…å®¹ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        # æ¦‚è¿°ç›¸å…³çš„å…³é”®è¯
        overview_keywords = ["æ¦‚è¿°", "åŠŸèƒ½è¯´æ˜", "åŠŸèƒ½æè¿°", "ç³»ç»Ÿè¯´æ˜", "é¡¹ç›®è¯´æ˜", "ç®€ä»‹", "èƒŒæ™¯"]
        
        # æŸ¥æ‰¾åŒ…å«æ¦‚è¿°å…³é”®è¯çš„ç« èŠ‚
        for idx, item in enumerate(structured_content):
            if item.get("type") == "heading":
                heading_text = item.get("text", "").strip()
                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«æ¦‚è¿°å…³é”®è¯
                if any(keyword in heading_text for keyword in overview_keywords):
                    # æ‰¾åˆ°æ¦‚è¿°ç« èŠ‚ï¼Œæ”¶é›†è¯¥ç« èŠ‚çš„å†…å®¹
                    content_parts = []
                    current_level = item.get("level", 1)
                    
                    # ä»ä¸‹ä¸€ä¸ªå…ƒç´ å¼€å§‹æ”¶é›†å†…å®¹ï¼Œç›´åˆ°é‡åˆ°åŒçº§æˆ–æ›´é«˜çº§åˆ«çš„æ ‡é¢˜
                    for j in range(idx + 1, len(structured_content)):
                        next_item = structured_content[j]
                        if next_item.get("type") == "heading":
                            next_level = next_item.get("level", 1)
                            if next_level <= current_level:
                                # é‡åˆ°åŒçº§æˆ–æ›´é«˜çº§åˆ«çš„æ ‡é¢˜ï¼Œåœæ­¢æ”¶é›†
                                break
                        
                        # æ”¶é›†æ®µè½å†…å®¹
                        if next_item.get("type") == "paragraph":
                            text = next_item.get("text", "").strip()
                            if text:
                                content_parts.append(text)
                    
                    # è¿”å›æ”¶é›†çš„å†…å®¹ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                    if content_parts:
                        full_content = "\n\n".join(content_parts)
                        # é™åˆ¶æœ€å¤§é•¿åº¦ä¸º500å­—ç¬¦
                        if len(full_content) > 500:
                            full_content = full_content[:500] + "..."
                        return full_content
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¦‚è¿°ç« èŠ‚ï¼Œå°è¯•è¿”å›ç¬¬ä¸€ä¸ªæ®µè½å†…å®¹ä½œä¸ºæ¦‚è¿°
        for item in structured_content:
            if item.get("type") == "paragraph":
                text = item.get("text", "").strip()
                # æ’é™¤å¤ªçŸ­çš„æ®µè½ï¼ˆå¦‚æ ‡é¢˜è¡Œï¼‰
                if text and len(text) > 30:
                    if len(text) > 500:
                        return text[:500] + "..."
                    return text
        
        return None
    
    @staticmethod
    def _build_summary_content(doc_data: Dict, sections: List[Dict], document_id: str = None, upload_id: int = None, file_name: str = None) -> str:
        """
        æ„å»ºæ€»ç»“æ–‡æ¡£ï¼ˆåŒ…å«å›¾ç‰‡æ¸…å•ã€è¡¨æ ¼æ¸…å•ã€åµŒå…¥æ–‡æ¡£æ¸…å•ç­‰ï¼‰
        
        ç”¨äºç”¨æˆ·æŸ¥çœ‹æ–‡æ¡£ç»“æ„æ¦‚è§ˆï¼Œä¸ç”¨äºGraphitiå¤„ç†
        
        Args:
            doc_data: è§£æåçš„æ–‡æ¡£æ•°æ®
            sections: ç« èŠ‚åˆ—è¡¨
            document_id: æ–‡æ¡£IDï¼ˆæ ¼å¼ï¼šupload_{upload_id}ï¼‰
            upload_id: ä¸Šä¼ IDï¼ˆç”¨äºæ„å»ºURLï¼‰
            file_name: æ–‡ä»¶åï¼ˆç”¨äºæ˜¾ç¤ºæ–‡æ¡£æ ‡é¢˜ï¼‰
        """
        # ä¼˜å…ˆä½¿ç”¨upload_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»document_idä¸­æå–
        if upload_id is None and document_id:
            if document_id.startswith("upload_"):
                try:
                    upload_id = int(document_id.replace("upload_", ""))
                except ValueError:
                    upload_id = None
        
        summary = "# æ–‡æ¡£è§£ææ€»ç»“\n\n"
        
        # æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
        metadata = doc_data.get("metadata", {})
        structured_content = doc_data.get("structured_content", [])
        
        # å°è¯•ä»æ–‡æ¡£å†…å®¹ä¸­æå–ä½œè€…ä¿¡æ¯ï¼ˆä¼˜å…ˆä»"æ–‡æ¡£ä¿®æ”¹è®°å½•"è¡¨æ ¼ä¸­æå–ï¼‰
        author_from_content = WordDocumentService._extract_author_from_content(doc_data)
        
        summary += "## æ–‡æ¡£æ¦‚è§ˆ\n\n"
        
        # åŸºæœ¬ä¿¡æ¯
        summary += "### åŸºæœ¬ä¿¡æ¯\n\n"
        # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ–‡æ¡£æ ‡é¢˜ï¼ˆå»æ‰æ‰©å±•åï¼‰
        if file_name:
            # å»æ‰æ–‡ä»¶æ‰©å±•å
            doc_title = file_name.rsplit('.', 1)[0] if '.' in file_name else file_name
        else:
            doc_title = metadata.get('title', 'æœªå‘½åæ–‡æ¡£')
        summary += f"- **æ–‡æ¡£æ ‡é¢˜**: {doc_title}\n"
        if author_from_content:
            summary += f"- **ä½œè€…**: {author_from_content}\n"
        if metadata.get('created'):
            created = metadata.get('created')
            if isinstance(created, datetime):
                summary += f"- **åˆ›å»ºæ—¶é—´**: {created.strftime('%Y-%m-%d %H:%M:%S')}\n"
            else:
                summary += f"- **åˆ›å»ºæ—¶é—´**: {created}\n"
        if metadata.get('modified'):
            modified = metadata.get('modified')
            if isinstance(modified, datetime):
                summary += f"- **ä¿®æ”¹æ—¶é—´**: {modified.strftime('%Y-%m-%d %H:%M:%S')}\n"
            else:
                summary += f"- **ä¿®æ”¹æ—¶é—´**: {modified}\n"
        summary += "\n"
        
        # ç« èŠ‚ç»“æ„ï¼ˆå¸¦å±‚çº§çš„ç›®å½•ï¼‰
        summary += "### ç« èŠ‚ç»“æ„\n\n"
        summary += "æœ¬æ–‡æ¡£åŒ…å«ä»¥ä¸‹ä¸»è¦ç« èŠ‚ï¼š\n\n"
        for item in structured_content:
            if item.get("type") == "heading":
                level = item.get("level", 1)
                heading_text = item.get("text", "").strip()
                if heading_text:
                    indent = "  " * (level - 1)
                    summary += f"{indent}- {heading_text}\n"
        summary += "\n"
        
        # åŠŸèƒ½æ¦‚è¿°ï¼ˆæŸ¥æ‰¾ç¬¬ä¸€ä¸ª"æ¦‚è¿°"ç›¸å…³ç« èŠ‚çš„å†…å®¹ï¼‰
        overview_content = WordDocumentService._extract_overview_content(structured_content)
        if overview_content:
            summary += "### åŠŸèƒ½æ¦‚è¿°\n\n"
            summary += overview_content + "\n\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        summary += "## ç»Ÿè®¡ä¿¡æ¯\n\n"
        summary += f"- **ç« èŠ‚æ•°**: {len(sections)}\n"
        summary += f"- **å›¾ç‰‡æ•°**: {len(doc_data.get('images', []))}\n"
        summary += f"- **è¡¨æ ¼æ•°**: {len(doc_data.get('tables', []))}\n"
        summary += f"- **é“¾æ¥æ•°**: {len(doc_data.get('links', []))}\n"
        summary += f"- **åµŒå…¥æ–‡æ¡£æ•°**: {len(doc_data.get('ole_objects', []))}\n"
        summary += f"- **æ–‡æœ¬é•¿åº¦**: {len(doc_data.get('text_content', ''))} å­—ç¬¦\n"
        summary += "\n"
        
        # å›¾ç‰‡æ¸…å•
        images = doc_data.get("images", [])
        if images:
            summary += "## å›¾ç‰‡æ¸…å•\n\n"
            for idx, image in enumerate(images, 1):
                image_id = image.get("image_id", f"image_{idx}")
                image_desc = image.get("description", "å›¾ç‰‡")
                section_title = image.get("section_title", "æœªçŸ¥ç« èŠ‚")
                relative_position = image.get("relative_position", 0.0)
                prev_context = image.get("prev_context", "")
                next_context = image.get("next_context", "")
                image_context = image.get("context", "")
                
                summary += f"### å›¾ç‰‡ {idx} ({image_id})\n\n"
                summary += f"- **æè¿°**: {image_desc}\n"
                summary += f"- **ä½ç½®**: {section_title} (æ–‡æ¡£ä½ç½®: {relative_position:.1%})\n"
                
                if upload_id:
                    image_url = f"/api/document-upload/{upload_id}/images/{image_id}"
                    summary += f"- **é“¾æ¥**: [æŸ¥çœ‹å›¾ç‰‡]({image_url})\n"
                
                # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
                summary += "\n**ä¸Šä¸‹æ–‡ä¿¡æ¯**:\n"
                if prev_context:
                    summary += f"- **å‰æ–‡**: {prev_context}\n"
                if image_context:
                    summary += f"- **å½“å‰æ®µè½**: {image_context}\n"
                if next_context:
                    summary += f"- **åæ–‡**: {next_context}\n"
                if not prev_context and not image_context and not next_context:
                    summary += "- æ— ä¸Šä¸‹æ–‡ä¿¡æ¯\n"
                
                summary += "\n"
        
        # è¡¨æ ¼æ¸…å•ï¼ˆä¸åŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        tables = doc_data.get("tables", [])
        if tables:
            summary += "## è¡¨æ ¼æ¸…å•\n\n"
            for idx, table_data in enumerate(tables, 1):
                table_id = table_data.get("table_id", f"table_{idx}")
                headers = table_data.get("headers", [])
                rows = table_data.get("rows", [])
                
                # æŸ¥æ‰¾è¡¨æ ¼æ‰€åœ¨çš„ç« èŠ‚
                section_title = "æœªçŸ¥ç« èŠ‚"
                structured_content = doc_data.get("structured_content", [])
                # å¯ä»¥é€šè¿‡structured_contentæŸ¥æ‰¾è¡¨æ ¼æ‰€åœ¨çš„ç« èŠ‚
                for item_idx, item in enumerate(structured_content):
                    if item.get("type") == "table" and item.get("table_id") == table_id:
                        # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„ç« èŠ‚æ ‡é¢˜
                        for i in range(item_idx, -1, -1):
                            prev_item = structured_content[i]
                            if prev_item.get("type") == "heading" and prev_item.get("level", 1) == 1:
                                section_title = prev_item.get("text", "æœªçŸ¥ç« èŠ‚")
                                break
                        break
                
                summary += f"### è¡¨æ ¼ {idx} ({table_id})\n\n"
                summary += f"- **ä½ç½®**: {section_title}\n"
                summary += f"- **è¡Œåˆ—æ•°**: {len(rows)} è¡Œ Ã— {len(headers)} åˆ—\n"
                
                # å†…å®¹æ‘˜è¦ï¼ˆå‰3è¡Œï¼‰
                if headers and rows:
                    summary += "\n**å†…å®¹æ‘˜è¦**:\n\n"
                    # è¡¨å¤´
                    header_row = "| " + " | ".join(str(h) for h in headers) + " |\n"
                    separator = "| " + " | ".join(["---"] * len(headers)) + " |\n"
                    summary += header_row + separator
                    # å‰3è¡Œæ•°æ®
                    for row in rows[:3]:
                        row_str = "| " + " | ".join(str(cell) for cell in row) + " |\n"
                        summary += row_str
                    if len(rows) > 3:
                        summary += f"| ... (è¿˜æœ‰ {len(rows) - 3} è¡Œ) |\n"
                    summary += "\n"
                
                summary += "\n"
        
        # åµŒå…¥æ–‡æ¡£æ¸…å•
        ole_objects = doc_data.get("ole_objects", [])
        if ole_objects:
            summary += "## åµŒå…¥æ–‡æ¡£æ¸…å•\n\n"
            for idx, ole in enumerate(ole_objects, 1):
                ole_id = ole.get("ole_id", f"ole_{idx}")
                ole_name = ole.get("name", "åµŒå…¥æ–‡æ¡£")
                ole_type = ole.get("type", "æœªçŸ¥ç±»å‹")
                
                summary += f"### åµŒå…¥æ–‡æ¡£ {idx} ({ole_id})\n\n"
                summary += f"- **åç§°**: {ole_name}\n"
                summary += f"- **ç±»å‹**: {ole_type}\n"
                
                if upload_id:
                    preview_url = f"/api/document-upload/{upload_id}/ole/{ole_id}?view=preview"
                    download_url = f"/api/document-upload/{upload_id}/ole/{ole_id}?view=download"
                    summary += f"- **é“¾æ¥**: [æŸ¥çœ‹]({preview_url}) | [ä¸‹è½½]({download_url})\n"
                
                summary += "\n"
        
        # é“¾æ¥æ¸…å•ï¼ˆå¯é€‰ï¼‰
        links = doc_data.get("links", [])
        if links:
            summary += "## é“¾æ¥æ¸…å•\n\n"
            seen_links = set()
            for link in links:
                link_key = (link.get('url', ''), link.get('text', ''))
                if link_key not in seen_links:
                    seen_links.add(link_key)
                    url = link.get('url', '')
                    text = link.get('text', url)
                    link_type = link.get('type', 'external')
                    summary += f"- [{text}]({url}) ({link_type})\n"
            summary += "\n"
        
        return summary
    
    @staticmethod
    def _generate_document_summary(doc_data: Dict) -> str:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        summary = f"æ–‡æ¡£åç§°ï¼š{doc_data['metadata'].get('title', '')}\n"
        summary += f"ä½œè€…ï¼š{doc_data['metadata'].get('author', '')}\n"
        created = doc_data['metadata'].get('created')
        if created:
            summary += f"åˆ›å»ºæ—¶é—´ï¼š{created.strftime('%Y-%m-%d %H:%M:%S')}\n"
        summary += "\næ–‡æ¡£æ¦‚è§ˆï¼š\n"
        summary += doc_data["text_content"][:2000]  # å‰ 2000 å­—ç¬¦ä½œä¸ºæ‘˜è¦
        return summary
    
    @staticmethod
    async def process_word_document(
        file_path: str,
        document_name: str,
        provider: str = "qianwen",
        max_tokens_per_section: int = 8000
    ) -> Dict[str, Any]:
        """
        å¤„ç† Word æ–‡æ¡£ï¼Œåˆ›å»ºåˆ†å±‚ Episode
        
        Args:
            file_path: Word æ–‡æ¡£è·¯å¾„
            document_name: æ–‡æ¡£åç§°
            provider: LLM æä¾›å•†
            max_tokens_per_section: æ¯ä¸ªç« èŠ‚çš„æœ€å¤§ token æ•°
        
        Returns:
            å¤„ç†ç»“æœï¼ŒåŒ…å«æ‰€æœ‰ Episode UUID
        """
        try:
            # Step 1: è§£æ Word æ–‡æ¡£
            logger.info(f"å¼€å§‹è§£æ Word æ–‡æ¡£: {file_path}")
            doc_data = WordDocumentService._parse_word_document(file_path)
            logger.info(f"æ–‡æ¡£è§£æå®Œæˆ: {len(doc_data['structured_content'])} ä¸ªå…ƒç´ ")
            
            # Step 2: æŒ‰ç« èŠ‚åˆ†å—
            sections = WordDocumentService._split_by_sections(
                doc_data["structured_content"],
                max_tokens=max_tokens_per_section
            )
            logger.info(f"æ–‡æ¡£åˆ†ä¸º {len(sections)} ä¸ªç« èŠ‚")
            
            # Step 3: è·å– Graphiti å®ä¾‹
            graphiti = get_graphiti_instance(provider)
            
            # æå–åŸºç¡€æ ‡è¯†å’Œç‰ˆæœ¬å·
            base_name = WordDocumentService._extract_base_name(document_name)
            version, version_number = WordDocumentService._extract_version(document_name)
            
            # æ¸…ç†åŸºç¡€æ ‡è¯†ï¼Œåªä¿ç•™å­—æ¯æ•°å­—ã€ç ´æŠ˜å·å’Œä¸‹åˆ’çº¿
            # Graphiti è¦æ±‚ group_id åªèƒ½åŒ…å« alphanumeric characters, dashes, or underscores
            safe_base_name = WordDocumentService._sanitize_group_id(base_name)
            
            # ä½¿ç”¨æ–‡æ¡£åˆ›å»ºæ—¥æœŸæˆ–å½“å‰æ—¥æœŸ
            doc_date = doc_data["metadata"].get("created")
            if doc_date and isinstance(doc_date, datetime):
                date_str = doc_date.strftime('%Y%m%d')
            else:
                date_str = datetime.now().strftime('%Y%m%d')
            
            # ç”ŸæˆåŸºç¡€ group_idï¼ˆæ‰€æœ‰ç‰ˆæœ¬å…±äº«ï¼‰
            group_id = f"doc_{safe_base_name}_{date_str}"
            logger.info(f"æ–‡æ¡£åŸºç¡€æ ‡è¯†: {base_name}")
            logger.info(f"æ–‡æ¡£ç‰ˆæœ¬: {version} (ç‰ˆæœ¬å·: {version_number})")
            logger.info(f"æ–‡æ¡£ group_id: {group_id}")
            
            # Step 4: åˆ›å»ºæ–‡æ¡£çº§ Episodeï¼ˆæå–æ–‡æ¡£çº§åˆ«çš„å®ä½“ï¼‰
            document_summary = WordDocumentService._generate_document_summary(doc_data)
            document_episode = await graphiti.add_episode(
                name=f"{document_name}_æ–‡æ¡£æ¦‚è§ˆ",
                episode_body=document_summary,
                source_description="Wordæ–‡æ¡£",
                reference_time=doc_data["metadata"].get("created") or datetime.now(),
                entity_types={
                    "Requirement": ENTITY_TYPES.get("Requirement"),
                    "Document": ENTITY_TYPES.get("Document"),
                } if ENTITY_TYPES.get("Requirement") and ENTITY_TYPES.get("Document") else None,
                group_id=group_id
            )
            document_episode_uuid = document_episode.episode.uuid
            logger.info(f"æ–‡æ¡£çº§ Episode åˆ›å»ºå®Œæˆ: {document_episode_uuid}")
            
            # æ›´æ–°æ–‡æ¡£çº§ Episode çš„ç‰ˆæœ¬ä¿¡æ¯å’Œæ–‡ä»¶è·¯å¾„
            from app.core.neo4j_client import neo4j_client
            update_version_query = """
            MATCH (e:Episodic)
            WHERE e.uuid = $episode_uuid
            SET e.version = $version,
                e.version_number = $version_number,
                e.document_name = $document_name,
                e.file_path = $file_path,
                e.original_filename = $original_filename
            RETURN e.uuid as uuid
            """
            neo4j_client.execute_write(update_version_query, {
                "episode_uuid": document_episode_uuid,
                "version": version,
                "version_number": version_number,
                "document_name": document_name,
                "file_path": file_path,
                "original_filename": os.path.basename(file_path)
            })
            logger.info(f"å·²æ›´æ–°æ–‡æ¡£çº§ Episode ç‰ˆæœ¬ä¿¡æ¯å’Œæ–‡ä»¶è·¯å¾„: version={version}, version_number={version_number}, file_path={file_path}")
            
            # å®šä¹‰æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯çš„å‡½æ•°ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰
            def update_episode_version(episode_uuid: str):
                neo4j_client.execute_write(update_version_query, {
                    "episode_uuid": episode_uuid,
                    "version": version,
                    "version_number": version_number,
                    "document_name": document_name,
                    "file_path": file_path,
                    "original_filename": os.path.basename(file_path)
                })
            
            # Step 5: åˆ›å»ºç« èŠ‚çº§ Episode
            section_episodes = []
            for idx, section in enumerate(sections):
                section_content = WordDocumentService._build_section_content(
                    section, doc_data, idx
                )
                
                section_episode = await graphiti.add_episode(
                    name=f"{document_name}_ç« èŠ‚_{idx+1}_{section['title'][:20]}",
                    episode_body=section_content,
                    source_description="Wordæ–‡æ¡£ç« èŠ‚",
                    reference_time=doc_data["metadata"].get("created") or datetime.now(),
                    entity_types=ENTITY_TYPES,
                    edge_types=EDGE_TYPES,
                    edge_type_map=EDGE_TYPE_MAP,
                    group_id=group_id,
                    previous_episode_uuids=[document_episode_uuid]
                )
                
                section_episode_uuid = section_episode.episode.uuid
                section_episodes.append(section_episode_uuid)
                
                # æ›´æ–°ç« èŠ‚çº§ Episode çš„ç‰ˆæœ¬ä¿¡æ¯
                update_episode_version(section_episode_uuid)
                
                logger.info(f"ç« èŠ‚ {idx+1} Episode åˆ›å»ºå®Œæˆ: {section_episode_uuid}")
            
            # Step 6: å¤„ç†å›¾ç‰‡ï¼Œä¸ºæ¯å¼ å›¾ç‰‡åˆ›å»ºç‹¬ç«‹çš„Episode
            image_episodes = []
            if doc_data["images"]:
                logger.info(f"å¼€å§‹å¤„ç† {len(doc_data['images'])} å¼ å›¾ç‰‡")
                for idx, image in enumerate(doc_data["images"]):
                    image_id = image.get("image_id", f"image_{idx+1}")
                    image_desc = image.get("description", f"å›¾ç‰‡ {idx+1}")
                    image_context = image.get("context", "")
                    image_url = f"/api/word-document/{group_id}/images/{image_id}"
                    
                    # è·å–å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                    prev_context = image.get('prev_context', '')
                    next_context = image.get('next_context', '')
                    section_title = image.get('section_title', '')
                    relative_position = image.get('relative_position', 0.0)
                    match_method = image.get('match_method', 'unknown')
                    match_confidence = image.get('match_confidence', 0.0)
                    file_size = image.get('file_size', 0)
                    file_format = image.get('file_format', 'UNKNOWN')
                    
                    # æ„å»ºå›¾ç‰‡Episodeçš„å†…å®¹ï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«æ›´å¤šå…ƒæ•°æ®å’Œä¸Šä¸‹æ–‡ï¼‰
                    image_content = f"""## å›¾ç‰‡ä¿¡æ¯

**å›¾ç‰‡ID**: {image_id}
**æè¿°**: {image_desc}
**æ–‡ä»¶è·¯å¾„**: {image.get('file_path', '')}
**ç›¸å¯¹è·¯å¾„**: {image.get('relative_path', '')}
**æ–‡ä»¶å¤§å°**: {file_size} å­—èŠ‚
**æ–‡ä»¶æ ¼å¼**: {file_format}
**åŒ¹é…æ–¹æ³•**: {match_method}
**åŒ¹é…ç½®ä¿¡åº¦**: {match_confidence:.2f}
**æ–‡æ¡£ä½ç½®**: {relative_position:.1%}
"""
                    
                    # æ·»åŠ ç« èŠ‚ä¿¡æ¯
                    if section_title:
                        image_content += f"**æ‰€å±ç« èŠ‚**: {section_title}\n\n"
                    
                    # æ·»åŠ å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                    image_content += "### ä¸Šä¸‹æ–‡ä¿¡æ¯\n\n"
                    if prev_context:
                        image_content += f"**å‰æ–‡**: {prev_context}\n\n"
                    if image_context:
                        image_content += f"**å½“å‰æ®µè½**: {image_context}\n\n"
                    if next_context:
                        image_content += f"**åæ–‡**: {next_context}\n\n"
                    if not prev_context and not image_context and not next_context:
                        image_content += "æ— ä¸Šä¸‹æ–‡ä¿¡æ¯\n\n"
                    
                    image_content += f"""### å›¾ç‰‡é“¾æ¥
![{image_desc}]({image_url})

### å›¾ç‰‡è¯´æ˜
è¿™æ˜¯ä¸€å¼ ä»Wordæ–‡æ¡£ä¸­æå–çš„å›¾ç‰‡ï¼Œä½äºæ–‡æ¡£çš„ç›¸åº”ä½ç½®ï¼ˆä½ç½®: {relative_position:.1%}ï¼‰ã€‚å›¾ç‰‡å¯èƒ½åŒ…å«æµç¨‹å›¾ã€ç¤ºæ„å›¾ã€å›¾è¡¨æˆ–å…¶ä»–å¯è§†åŒ–å†…å®¹ã€‚

**åŒ¹é…ä¿¡æ¯**: é€šè¿‡{match_method}æ–¹æ³•åŒ¹é…ï¼Œç½®ä¿¡åº¦ä¸º{match_confidence:.0%}ã€‚
"""
                    
                    # åˆ›å»ºå›¾ç‰‡Episode
                    image_episode = await graphiti.add_episode(
                        name=f"{document_name}_å›¾ç‰‡_{idx+1}_{image_desc[:20]}",
                        episode_body=image_content,
                        source_description="Wordæ–‡æ¡£å›¾ç‰‡",
                        reference_time=doc_data["metadata"].get("created") or datetime.now(),
                        entity_types=ENTITY_TYPES,
                        edge_types=EDGE_TYPES,
                        edge_type_map=EDGE_TYPE_MAP,
                        group_id=group_id,
                        previous_episode_uuids=[document_episode_uuid]
                    )
                    
                    image_episode_uuid = image_episode.episode.uuid
                    image_episodes.append(image_episode_uuid)
                    
                    # æ›´æ–°å›¾ç‰‡ Episode çš„ç‰ˆæœ¬ä¿¡æ¯
                    update_episode_version(image_episode_uuid)
                    
                    logger.info(f"å›¾ç‰‡ {idx+1} Episode åˆ›å»ºå®Œæˆ: {image_episode_uuid}")
            
            # Step 7: å¤„ç†è¡¨æ ¼ï¼Œä¸ºæ¯ä¸ªè¡¨æ ¼åˆ›å»ºç‹¬ç«‹çš„Episode
            table_episodes = []
            if doc_data["tables"]:
                logger.info(f"å¼€å§‹å¤„ç† {len(doc_data['tables'])} ä¸ªè¡¨æ ¼")
                for idx, table_data in enumerate(doc_data["tables"]):
                    # æ ¼å¼åŒ–è¡¨æ ¼ä¸ºæ ‡å‡†Markdownæ ¼å¼ï¼ˆç”¨äºEpisodeå†…å®¹ï¼‰
                    table_markdown = WordDocumentService._format_table_as_markdown(table_data)
                    
                    # æ„å»ºè¡¨æ ¼Episodeçš„å†…å®¹
                    table_content = f"""## è¡¨æ ¼ä¿¡æ¯

**è¡¨æ ¼åºå·**: {idx+1}
**è¡¨æ ¼ID**: {table_data.get('table_id', f'table_{idx+1}')}
**è¡Œæ•°**: {len(table_data.get('rows', []))}
**åˆ—æ•°**: {len(table_data.get('headers', []))}

### è¡¨æ ¼å†…å®¹

{table_markdown}

### è¡¨æ ¼è¯´æ˜
è¿™æ˜¯ä»Wordæ–‡æ¡£ä¸­æå–çš„è¡¨æ ¼æ•°æ®ï¼Œä½¿ç”¨æ ‡å‡†Markdownè¡¨æ ¼æ ¼å¼ï¼ŒåŒ…å«ç»“æ„åŒ–çš„ä¿¡æ¯ã€‚
"""
                    
                    # åˆ›å»ºè¡¨æ ¼Episode
                    table_episode = await graphiti.add_episode(
                        name=f"{document_name}_è¡¨æ ¼_{idx+1}",
                        episode_body=table_content,
                        source_description="Wordæ–‡æ¡£è¡¨æ ¼",
                        reference_time=doc_data["metadata"].get("created") or datetime.now(),
                        entity_types=ENTITY_TYPES,
                        edge_types=EDGE_TYPES,
                        edge_type_map=EDGE_TYPE_MAP,
                        group_id=group_id,
                        previous_episode_uuids=[document_episode_uuid]
                    )
                    
                    table_episode_uuid = table_episode.episode.uuid
                    table_episodes.append(table_episode_uuid)
                    
                    # æ›´æ–°è¡¨æ ¼ Episode çš„ç‰ˆæœ¬ä¿¡æ¯
                    update_episode_version(table_episode_uuid)
                    
                    logger.info(f"è¡¨æ ¼ {idx+1} Episode åˆ›å»ºå®Œæˆ: {table_episode_uuid}")
            
            return {
                "success": True,
                "document_id": group_id,
                "document_name": document_name,
                "document_episode_uuid": document_episode_uuid,
                "section_episodes": section_episodes,
                "image_episodes": image_episodes,
                "table_episodes": table_episodes,
                "statistics": {
                    "total_sections": len(sections),
                    "total_images": len(doc_data["images"]),
                    "total_tables": len(doc_data["tables"]),
                    "total_links": len(doc_data["links"])
                }
            }
        except Exception as e:
            logger.error(f"å¤„ç† Word æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
            raise

