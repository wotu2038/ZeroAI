from fastapi import APIRouter, HTTPException, Query
from typing import List, Optional, Dict, Any
from app.models.schemas import (
    Requirement, RequirementCreate, RequirementUpdate,
    SimilarRequirementQuery, SimilarRequirementResponse,
    RequirementDocumentGenerateRequest, RequirementDocumentGenerateResponse,
    RequirementDocumentGenerateAsyncRequest,
    LLMChatRequest, LLMResponse
)
from app.services.requirement_service import RequirementService
from app.services.graphiti_service import GraphitiService
from app.services.conversation_service import ConversationService
from app.core.llm_client import llm_client
from app.core.neo4j_client import neo4j_client
from app.core.mysql_client import get_db
from app.core.auth import get_current_user_optional
from app.models.task_queue import TaskQueue, TaskStatus, TaskType
from app.models.chat_history import ChatHistory, ChatMode
from app.models.user import User
from app.tasks.requirement_generation import generate_requirement_document_task
from sqlalchemy.orm import Session
from fastapi import Depends
from datetime import datetime
import logging
import time
import uuid

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/requirements", tags=["éœ€æ±‚åŠ©æ‰‹"])


@router.post("", response_model=Requirement, status_code=201)
async def create_requirement(
    requirement: RequirementCreate,
    provider: str = Query("qianwen", description="LLMæä¾›å•†")
):
    """
    åˆ›å»ºéœ€æ±‚æ–‡æ¡£
    
    ä½¿ç”¨æ–¹æ¡ˆCï¼ˆæ··åˆæ–¹æ¡ˆï¼‰ï¼š
    1. Graphiti åˆæ­¥æå–
    2. è‡ªå®šä¹‰ Prompt ç»“æ„åŒ–æå–
    3. åˆå¹¶å’Œå¢å¼º
    """
    try:
        result = await RequirementService.create_requirement(requirement, provider)
        return result
    except Exception as e:
        logger.error(f"åˆ›å»ºéœ€æ±‚å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºéœ€æ±‚å¤±è´¥: {str(e)}")


@router.get("/{requirement_id}", response_model=Requirement)
async def get_requirement(requirement_id: str):
    """è·å–éœ€æ±‚è¯¦æƒ…"""
    requirement = RequirementService.get_requirement(requirement_id)
    if not requirement:
        raise HTTPException(status_code=404, detail="éœ€æ±‚ä¸å­˜åœ¨")
    return requirement


@router.get("", response_model=List[Requirement])
async def list_requirements(
    limit: int = Query(50, ge=1, le=100, description="è¿”å›æ•°é‡é™åˆ¶"),
    offset: int = Query(0, ge=0, description="åç§»é‡")
):
    """è·å–éœ€æ±‚åˆ—è¡¨"""
    try:
        requirements = RequirementService.list_requirements(limit=limit, offset=offset)
        return requirements
    except Exception as e:
        logger.error(f"è·å–éœ€æ±‚åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–éœ€æ±‚åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.put("/{requirement_id}", response_model=Requirement)
async def update_requirement(
    requirement_id: str,
    requirement: RequirementUpdate
):
    """æ›´æ–°éœ€æ±‚"""
    # TODO: å®ç°æ›´æ–°é€»è¾‘
    raise HTTPException(status_code=501, detail="æ›´æ–°åŠŸèƒ½å¾…å®ç°")


@router.delete("/{requirement_id}", status_code=204)
async def delete_requirement(requirement_id: str):
    """åˆ é™¤éœ€æ±‚"""
    # TODO: å®ç°åˆ é™¤é€»è¾‘
    raise HTTPException(status_code=501, detail="åˆ é™¤åŠŸèƒ½å¾…å®ç°")


@router.post("/similar", response_model=SimilarRequirementResponse)
async def find_similar_requirements(
    query: SimilarRequirementQuery,
    provider: str = Query("qianwen", description="LLMæä¾›å•†")
):
    """
    æŸ¥æ‰¾ç›¸ä¼¼éœ€æ±‚ï¼ˆç»„åˆæ–¹æ¡ˆï¼‰
    
    æ”¯æŒä¸¤ç§æŸ¥è¯¢æ–¹å¼ï¼š
    1. åŸºäºéœ€æ±‚IDï¼šæŸ¥è¯¢ä¸æŒ‡å®šéœ€æ±‚ç›¸ä¼¼çš„å…¶ä»–éœ€æ±‚
    2. åŸºäºæ–‡æœ¬ï¼šåŸºäºæŸ¥è¯¢æ–‡æœ¬æŸ¥æ‰¾ç›¸ä¼¼éœ€æ±‚
    
    ç»„åˆæŸ¥è¯¢åŒ…æ‹¬ï¼š
    - è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆGraphitiï¼‰
    - åŠŸèƒ½ç‚¹é‡åˆåº¦
    - æ¨¡å—é‡åˆåº¦
    """
    try:
        result = await RequirementService.find_similar_requirements(
            requirement_id=query.requirement_id,
            query_text=query.query_text,
            limit=query.limit,
            include_features=query.include_features,
            include_modules=query.include_modules,
            provider=provider
        )
        return result
    except Exception as e:
        logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼éœ€æ±‚å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥æ‰¾ç›¸ä¼¼éœ€æ±‚å¤±è´¥: {str(e)}")


@router.post("/generate", response_model=RequirementDocumentGenerateResponse)
async def generate_requirement_document(
    request: RequirementDocumentGenerateRequest,
    provider: str = Query("qianwen", description="LLMæä¾›å•†")
):
    """
    ç”Ÿæˆéœ€æ±‚æ–‡æ¡£ï¼ˆåŒæ­¥ï¼‰
    
    ç»“åˆæ–°éœ€æ±‚ + ç›¸ä¼¼å†å²éœ€æ±‚ï¼Œç”Ÿæˆæ–°æ–‡æ¡£
    
    æ”¯æŒæ ¼å¼ï¼š
    - markdownï¼ˆé»˜è®¤ï¼‰
    - wordï¼ˆå¾…å®ç°ï¼‰
    - pdfï¼ˆå¾…å®ç°ï¼‰
    """
    try:
        result = await RequirementService.generate_requirement_document(
            new_requirement_id=request.new_requirement_id,
            similar_requirement_ids=request.similar_requirement_ids,
            format=request.format,
            provider=provider
        )
        return RequirementDocumentGenerateResponse(**result)
    except Exception as e:
        logger.error(f"ç”Ÿæˆéœ€æ±‚æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆéœ€æ±‚æ–‡æ¡£å¤±è´¥: {str(e)}")


@router.post("/generate-async")
async def generate_requirement_document_async(
    request: RequirementDocumentGenerateAsyncRequest,
    db: Session = Depends(get_db)
):
    """
    å¼‚æ­¥ç”Ÿæˆéœ€æ±‚æ–‡æ¡£ï¼ˆä½¿ç”¨LangGraphå¤šAgentå·¥ä½œæµï¼‰
    
    å·¥ä½œæµç¨‹ï¼š
    1. æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢ç›¸å…³å†…å®¹ï¼ˆEpisodeã€Entityã€Edgeï¼Œå¿…è¦æ—¶Communityï¼‰
    2. æ•´åˆæ£€ç´¢ç»“æœå’Œç›¸ä¼¼éœ€æ±‚ï¼Œç”Ÿæˆåˆå§‹æ–‡æ¡£
    3. è¯„å®¡æ–‡æ¡£è´¨é‡
    4. æ ¹æ®è¯„å®¡ç»“æœä¼˜åŒ–æ–‡æ¡£ï¼ˆè¿­ä»£ï¼‰
    5. è¾“å‡ºæœ€ç»ˆæ–‡æ¡£
    
    æ”¯æŒé…ç½®ï¼š
    - æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆ1-10ï¼‰
    - è´¨é‡é˜ˆå€¼ï¼ˆ0-100ï¼‰
    - æ£€ç´¢ç»“æœæ•°é‡é™åˆ¶ï¼ˆ5-50ï¼‰
    - æ£€ç´¢æ¨¡å¼ï¼ˆå•æ–‡æ¡£/å¤šæ–‡æ¡£/å…¨éƒ¨æ–‡æ¡£ï¼‰
    """
    try:
        # å…ˆç”ŸæˆCeleryä»»åŠ¡IDï¼ˆé¿å…ç©ºå­—ç¬¦ä¸²å¯¼è‡´å”¯ä¸€é”®å†²çªï¼‰
        celery_task_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        task = TaskQueue(
            task_id=celery_task_id,  # ç›´æ¥ä½¿ç”¨ç”Ÿæˆçš„ID
            upload_id=0,  # éœ€æ±‚æ–‡æ¡£ç”Ÿæˆä¸éœ€è¦upload_id
            task_type=TaskType.GENERATE_REQUIREMENT_DOCUMENT.value,
            status=TaskStatus.PENDING.value,
            progress=0,
            current_step="ç­‰å¾…å¤„ç†"
        )
        db.add(task)
        db.commit()
        db.refresh(task)
        
        # æäº¤Celeryä»»åŠ¡
        try:
            celery_task = generate_requirement_document_task.delay(
                task_id=celery_task_id,
                user_query=request.user_query,
                new_requirement_id=request.new_requirement_id,
                similar_requirement_ids=request.similar_requirement_ids,
                format=request.format,
                max_iterations=request.max_iterations,
                quality_threshold=request.quality_threshold,
                retrieval_limit=request.retrieval_limit,
                group_id=request.group_id,
                group_ids=request.group_ids,
                all_documents=request.all_documents,
                use_thinking=request.use_thinking
            )
            
            # éªŒè¯Celeryä»»åŠ¡ID
            if not celery_task or not celery_task.id:
                logger.error(f"Celeryä»»åŠ¡æäº¤å¤±è´¥ï¼šæœªè¿”å›ä»»åŠ¡ID")
                raise ValueError("Celeryä»»åŠ¡æäº¤å¤±è´¥ï¼šæœªè¿”å›ä»»åŠ¡ID")
            
            # å¦‚æœCeleryè¿”å›çš„IDä¸åŒï¼Œæ›´æ–°ä»»åŠ¡ID
            if celery_task.id != celery_task_id:
                task.task_id = celery_task.id
                db.commit()
            
            logger.info(f"ä»»åŠ¡æäº¤æˆåŠŸ: celery_task_id={celery_task.id}, db_task_id={celery_task_id}")
            
            return {
                "task_id": celery_task.id,
                "status": "pending",
                "message": "ä»»åŠ¡å·²æäº¤"
            }
        except Exception as celery_error:
            logger.error(f"æäº¤Celeryä»»åŠ¡å¤±è´¥: {celery_error}", exc_info=True)
            # å¦‚æœCeleryä»»åŠ¡æäº¤å¤±è´¥ï¼Œåˆ é™¤å·²åˆ›å»ºçš„ä»»åŠ¡è®°å½•
            try:
                db.delete(task)
                db.commit()
            except:
                pass
            raise HTTPException(status_code=500, detail=f"æäº¤Celeryä»»åŠ¡å¤±è´¥: {str(celery_error)}")
        
    except Exception as e:
        logger.error(f"æäº¤ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


# ==================== æ™ºèƒ½é—®ç­”ç›¸å…³ç«¯ç‚¹ ====================

@router.post("/qa/chat", response_model=LLMResponse)
async def qa_chat(
    request: LLMChatRequest,
    group_id: Optional[str] = Query(None, description="æ–‡æ¡£ group_idï¼ˆå•æ–‡æ¡£æ¨¡å¼ï¼‰"),
    group_ids: Optional[List[str]] = Query(None, description="æ–‡æ¡£ group_id åˆ—è¡¨ï¼ˆå¤šæ–‡æ¡£æ¨¡å¼ï¼‰"),
    all_documents: bool = Query(False, description="æ˜¯å¦æ£€ç´¢å…¨éƒ¨æ–‡æ¡£"),
    knowledge_base_id: Optional[int] = Query(None, description="çŸ¥è¯†åº“IDï¼ˆç”¨äºä¿å­˜å¯¹è¯å†å²ï¼‰"),
    session_id: Optional[str] = Query(None, description="ä¼šè¯IDï¼ˆç”¨äºMem0è®°å¿†ç®¡ç†ï¼‰"),
    current_user: Optional[User] = Depends(get_current_user_optional),
    db: Session = Depends(get_db)
):
    """
    æ™ºèƒ½é—®ç­”ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    æ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
    1. å•æ–‡æ¡£æ¨¡å¼ï¼šæŒ‡å®š group_id
    2. å¤šæ–‡æ¡£æ¨¡å¼ï¼šæŒ‡å®š group_ids åˆ—è¡¨
    3. å…¨éƒ¨æ–‡æ¡£æ¨¡å¼ï¼šè®¾ç½® all_documents=True
    
    å¢å¼ºåŠŸèƒ½ï¼š
    - æ–‡æ¡£é€ä¸ªæ€»ç»“
    - çŸ¥è¯†è¦†ç›–åº¦åˆ†æ
    - çŸ¥è¯†ç¼ºå£æç¤º
    - è¿½é—®å»ºè®®
    """
    retrieval_start_time = time.time()
    
    try:
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_messages = [msg for msg in request.messages if msg.get("role") == "user"]
        if not user_messages:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯")
        
        last_user_message = user_messages[-1]["content"]
        
        # ç¡®å®šæ£€ç´¢èŒƒå›´
        if all_documents:
            # å…¨éƒ¨æ–‡æ¡£æ¨¡å¼ï¼šä¸é™åˆ¶ group_id
            target_group_ids = None
            scope_description = "å…¨éƒ¨æ–‡æ¡£"
        elif group_ids:
            # å¤šæ–‡æ¡£æ¨¡å¼
            target_group_ids = group_ids
            scope_description = f"{len(group_ids)} ä¸ªæ–‡æ¡£"
        elif group_id:
            # å•æ–‡æ¡£æ¨¡å¼
            target_group_ids = [group_id]
            scope_description = f"æ–‡æ¡£ {group_id}"
        else:
            raise HTTPException(status_code=400, detail="å¿…é¡»æŒ‡å®š group_idã€group_ids æˆ–è®¾ç½® all_documents=True")
        
        # è·å–æ£€ç´¢æ–¹æ¡ˆï¼ˆä» cross_encoder_mode æ˜ å°„ï¼‰
        scheme_map = {
            "default": "default",
            "enhanced": "enhanced",
            "smart": "smart"
        }
        scheme = scheme_map.get(request.cross_encoder_mode if hasattr(request, 'cross_encoder_mode') else "default", "default")
        
        # ä½¿ç”¨ ConversationService è¿›è¡Œå¢å¼ºé—®ç­”
        # ä»è¯·æ±‚ä¸­è·å– providerï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œé»˜è®¤ä¸º "local"
        provider = getattr(request, 'provider', 'local') if hasattr(request, 'provider') else 'local'
        conversation_service = ConversationService(llm_client=llm_client, provider=provider)
        
        # å‡†å¤‡å¯¹è¯å†å²ï¼ˆæ’é™¤æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œå› ä¸ºå®ƒæ˜¯å½“å‰é—®é¢˜ï¼‰
        history = request.messages[:-1] if len(request.messages) > 1 else None
        
        # è·å–ç”¨æˆ·IDå’Œä¼šè¯IDï¼ˆç”¨äºMem0è®°å¿†ç®¡ç†ï¼‰
        user_id = str(current_user.id) if current_user else "anonymous"
        # å¦‚æœæ²¡æœ‰æä¾›session_idï¼Œç”Ÿæˆä¸€ä¸ªï¼ˆåŸºäºknowledge_base_idï¼‰
        if not session_id and knowledge_base_id:
            session_id = f"kb_{knowledge_base_id}"
        elif not session_id:
            import uuid
            session_id = str(uuid.uuid4())
        
        # è°ƒç”¨å¢å¼ºå¯¹è¯æœåŠ¡
        conversation_response = await conversation_service.chat(
            query=last_user_message,
            group_ids=target_group_ids,
            scheme=scheme,
            history=history,
            top_k=10,
            provider=request.provider,
            user_id=user_id,
            session_id=session_id
        )
        
        # å°† RetrievalResult è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå…¼å®¹å‰ç«¯æ ¼å¼ï¼‰
        retrieval_results = []
        if conversation_response.retrieval_results:
            for result in conversation_response.retrieval_results:
                # æ ¹æ® source_type æ„å»ºä¸åŒçš„æ ¼å¼
                result_dict = {
                    "type": result.source_type,  # entity, edge, episode, community
                    "id": result.uuid,
                    "score": result.score,
                    "properties": {
                        "name": result.name,
                        "content": result.content,
                        "group_id": result.group_id or ""
                    }
                }
                
                # æ ¹æ®ç±»å‹æ·»åŠ ç‰¹å®šå­—æ®µ
                if result.source_type == "entity":
                    result_dict["labels"] = ["Entity"]
                elif result.source_type == "edge":
                    result_dict["rel_type"] = result.name
                    # å¦‚æœæœ‰ metadataï¼Œæå– source_name å’Œ target_name
                    if result.metadata:
                        result_dict["source_name"] = result.metadata.get("source_name", "")
                        result_dict["target_name"] = result.metadata.get("target_name", "")
                elif result.source_type == "community":
                    # Community å¯èƒ½æœ‰ member_count ç­‰ä¿¡æ¯
                    if result.metadata:
                        result_dict["properties"]["member_count"] = result.metadata.get("member_count", 0)
                        result_dict["properties"]["summary"] = result.metadata.get("summary", "")
                        result_dict["properties"]["member_names"] = result.metadata.get("member_names", [])
                
                retrieval_results.append(result_dict)
        
        # æ„å»ºå¢å¼ºçš„å›ç­”å†…å®¹
        answer_content = conversation_response.answer
        
        # æ·»åŠ çŸ¥è¯†è¦†ç›–åº¦ä¿¡æ¯
        if conversation_response.coverage_analysis:
            coverage = conversation_response.coverage_analysis
            coverage_level_cn = {"high": "é«˜", "medium": "ä¸­", "low": "ä½"}.get(coverage.get("coverage_level", "low"), "ä½")
            answer_content += f"\n\nğŸ“Š **çŸ¥è¯†è¦†ç›–åº¦**: {coverage_level_cn} | åŸºäº{coverage.get('document_count', 0)}ä¸ªæ–‡æ¡£"
        
        # æ·»åŠ æ–‡æ¡£æ€»ç»“éƒ¨åˆ†
        if conversation_response.document_summaries:
            answer_content += "\n\n---\n\n### ğŸ“„ ç›¸å…³æ–‡æ¡£æ€»ç»“\n\n"
            for idx, doc_summary in enumerate(conversation_response.document_summaries[:5], 1):
                kb_prefix = f"ã€{doc_summary.knowledge_base_name}ã€‘" if doc_summary.knowledge_base_name else ""
                answer_content += f"ğŸ“„ [{idx}] {kb_prefix}{doc_summary.document_name} | ç›¸å…³åº¦: {doc_summary.relevance_score:.0f}%\n"
                answer_content += f"   â€¢ å…³ç³»: {doc_summary.relationship}\n"
                if doc_summary.key_content:
                    answer_content += f"   â€¢ é‡ç‚¹: {doc_summary.key_content[:100]}\n"
                if doc_summary.suggestion:
                    answer_content += f"   â€¢ å»ºè®®: {doc_summary.suggestion}\n"
                answer_content += f"   â€¢ [æŸ¥çœ‹æ–‡æ¡£]({doc_summary.preview_url})\n\n"
        
        # æ·»åŠ çŸ¥è¯†ç¼ºå£æç¤º
        if conversation_response.knowledge_gaps:
            answer_content += f"\nâš ï¸ **çŸ¥è¯†ç¼ºå£æç¤º**: çŸ¥è¯†åº“ä¸­æš‚æ— ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼Œå»ºè®®è¡¥å……ç›¸å…³æ–‡æ¡£ï¼š\n"
            for gap in conversation_response.knowledge_gaps:
                answer_content += f"   - {gap}\n"
        
        # æ·»åŠ è¿½é—®å»ºè®®
        if conversation_response.follow_up_questions:
            answer_content += "\n\n---\n\n### ğŸ’­ æ‚¨å¯èƒ½è¿˜æƒ³é—®\n\n"
            for question in conversation_response.follow_up_questions:
                answer_content += f"â€¢ {question}\n"
        
        retrieval_time = (time.time() - retrieval_start_time) * 1000
        
        # è½¬æ¢æ–‡æ¡£æ€»ç»“ä¸ºå­—å…¸æ ¼å¼
        document_summaries_dict = []
        for doc_summary in conversation_response.document_summaries:
            document_summaries_dict.append({
                "document_id": doc_summary.document_id,
                "document_name": doc_summary.document_name,
                "upload_id": doc_summary.upload_id,
                "knowledge_base_name": doc_summary.knowledge_base_name,
                "relationship": doc_summary.relationship,
                "key_content": doc_summary.key_content,
                "suggestion": doc_summary.suggestion,
                "relevance_score": doc_summary.relevance_score,
                "preview_url": doc_summary.preview_url,
                "has_smart_summary": doc_summary.has_smart_summary
            })
        
        logger.info(f"å¢å¼ºé—®ç­”å®Œæˆï¼ŒåŸºäº {conversation_response.retrieval_count} ä¸ªæ£€ç´¢ç»“æœï¼Œ{len(conversation_response.document_summaries)} ä¸ªæ–‡æ¡£æ€»ç»“")
        
        # ä¿å­˜å¯¹è¯å†å²åˆ°æ•°æ®åº“ï¼ˆå¦‚æœç”¨æˆ·å·²ç™»å½•ï¼‰
        if current_user:
            try:
                # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                user_message = last_user_message
                assistant_message = answer_content
                
                # æ„å»ºæ£€ç´¢æ‘˜è¦
                retrieval_summary = {
                    "count": conversation_response.retrieval_count,
                    "time_ms": retrieval_time,
                    "scheme": scheme,
                    "document_count": len(document_summaries_dict)
                }
                
                # ç¡®å®š group_idsï¼ˆç”¨äºå¤šæ–‡æ¡£æ¨¡å¼ï¼‰
                final_group_ids = None
                if group_ids:
                    final_group_ids = group_ids
                elif group_id:
                    final_group_ids = [group_id]
                
                # åˆ›å»ºå¯¹è¯å†å²è®°å½•
                chat_history = ChatHistory(
                    user_id=current_user.id,
                    knowledge_base_id=knowledge_base_id,
                    chat_mode=ChatMode.CONVERSATION,
                    group_ids=final_group_ids,
                    all_documents="true" if all_documents else "false",
                    retrieval_scheme=scheme,
                    provider=request.provider if hasattr(request, 'provider') else "local",
                    use_thinking="true" if (hasattr(request, 'use_thinking') and request.use_thinking) else "false",
                    user_message=user_message,
                    assistant_message=assistant_message,
                    retrieval_summary=retrieval_summary,
                    retrieval_results=retrieval_results if retrieval_results else None  # ä¿å­˜å®Œæ•´çš„æ£€ç´¢ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                )
                
                db.add(chat_history)
                db.commit()
                logger.info(f"å¯¹è¯å†å²å·²ä¿å­˜: user_id={current_user.id}, kb_id={knowledge_base_id}, retrieval_results_count={len(retrieval_results) if retrieval_results else 0}")
            except Exception as e:
                logger.warning(f"ä¿å­˜å¯¹è¯å†å²å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")
                db.rollback()
        
        return LLMResponse(
            content=answer_content,
            answer=answer_content,  # å…¼å®¹å‰ç«¯
            retrieval_results=retrieval_results,
            retrieval_count=conversation_response.retrieval_count,
            retrieval_time=retrieval_time,
            has_context=conversation_response.retrieval_count > 0,
            document_summaries=document_summaries_dict,
            knowledge_coverage=conversation_response.coverage_analysis,
            knowledge_gaps=conversation_response.knowledge_gaps,
            follow_up_questions=conversation_response.follow_up_questions
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"é—®ç­”å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"é—®ç­”å¤±è´¥: {str(e)}")


@router.get("/qa/history")
async def get_chat_history(
    knowledge_base_id: Optional[int] = Query(None, description="çŸ¥è¯†åº“IDï¼ˆå¯é€‰ï¼Œç”¨äºç­›é€‰ï¼‰"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    current_user: Optional[User] = Depends(get_current_user_optional),
    db: Session = Depends(get_db)
):
    """
    è·å–å¯¹è¯å†å²è®°å½•
    
    å¦‚æœç”¨æˆ·æœªç™»å½•ï¼Œè¿”å›ç©ºåˆ—è¡¨
    """
    if not current_user:
        return {"items": [], "total": 0, "page": page, "page_size": page_size}
    
    try:
        # æ„å»ºæŸ¥è¯¢
        query = db.query(ChatHistory).filter(ChatHistory.user_id == current_user.id)
        
        # å¦‚æœæŒ‡å®šäº†çŸ¥è¯†åº“IDï¼Œè¿›è¡Œç­›é€‰
        if knowledge_base_id:
            query = query.filter(ChatHistory.knowledge_base_id == knowledge_base_id)
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        query = query.order_by(ChatHistory.created_at.desc())
        
        # åˆ†é¡µ
        total = query.count()
        histories = query.offset((page - 1) * page_size).limit(page_size).all()
        
        # è½¬æ¢ä¸ºå­—å…¸
        result = [h.to_dict() for h in histories]
        
        return {
            "items": result,
            "total": total,
            "page": page,
            "page_size": page_size
        }
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯å†å²å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")


@router.post("/qa/similar")
async def qa_similar_requirements(
    query_text: str = Query(..., description="æŸ¥è¯¢æ–‡æœ¬"),
    group_id: Optional[str] = Query(None, description="å½“å‰æ–‡æ¡£ group_idï¼ˆå¯é€‰ï¼Œç”¨äºæ’é™¤è‡ªå·±ï¼‰"),
    limit: int = Query(5, ge=1, le=20, description="è¿”å›ç»“æœæ•°é‡"),
    provider: str = Query("qianwen", description="LLMæä¾›å•†")
):
    """
    ç›¸ä¼¼éœ€æ±‚æ¨è
    
    æ ¹æ®é—®é¢˜æè¿°ï¼Œæ™ºèƒ½æ¨èç›¸ä¼¼çš„å†å²éœ€æ±‚æ–‡æ¡£
    """
    try:
        from app.core.neo4j_client import neo4j_client
        from app.core.graphiti_client import get_graphiti_instance
        
        # ä½¿ç”¨ Graphiti è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œç›´æ¥æœç´¢æ–‡æ¡£çº§ Episode
        logger.info(f"å¼€å§‹æŸ¥æ‰¾ç›¸ä¼¼éœ€æ±‚: query='{query_text}'")
        
        # æŸ¥è¯¢æ‰€æœ‰æ–‡æ¡£çº§ Episode
        episode_query = """
        MATCH (e:Episodic)
        WHERE e.name CONTAINS 'æ–‡æ¡£æ¦‚è§ˆ'
        RETURN DISTINCT e.group_id as group_id, 
               e.document_name as document_name,
               e.version as version, 
               e.created_at as created_at,
               e.content as content,
               e.uuid as episode_uuid
        ORDER BY e.created_at DESC
        """
        all_episodes = neo4j_client.execute_query(episode_query)
        logger.info(f"æŸ¥è¯¢åˆ° {len(all_episodes)} ä¸ªæ–‡æ¡£çº§Episode")
        
        if not all_episodes:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£çº§Episode")
            return {
                "query": query_text,
                "similar_documents": [],
                "count": 0
            }
        
        # ä½¿ç”¨ Graphiti è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œè·å–ç›¸å…³çš„Entityå’ŒEdge
        graphiti = get_graphiti_instance(provider)
        related_group_ids = set()
        
        try:
            search_results = await graphiti.search(query=query_text, num_results=limit * 3)
            logger.info(f"è¯­ä¹‰æœç´¢æˆåŠŸï¼Œè¿”å› {len(search_results) if search_results else 0} ä¸ªç»“æœ")
        
            # ä»æœç´¢ç»“æœä¸­æå–ç›¸å…³çš„group_id
            # Graphitiè¿”å›çš„æ˜¯EntityEdgeï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¿™äº›Edgeå…³è”çš„Episodeçš„group_id
            for result in search_results:
                # è·å–æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹çš„UUID
                source_uuid = getattr(result, 'source_node_uuid', None)
                target_uuid = getattr(result, 'target_node_uuid', None)
                
                # æŸ¥è¯¢è¿™äº›èŠ‚ç‚¹å…³è”çš„Episodeçš„group_id
                for node_uuid in [source_uuid, target_uuid]:
                    if node_uuid:
                        node_episode_query = """
                        MATCH (n {uuid: $uuid})<-[:MENTIONS]-(e:Episodic)
                        WHERE e.name CONTAINS 'æ–‡æ¡£æ¦‚è§ˆ'
                        RETURN DISTINCT e.group_id as group_id
                        LIMIT 5
                        """
                        node_episodes = neo4j_client.execute_query(node_episode_query, {"uuid": str(node_uuid)})
                        for ep in node_episodes:
                            gid = ep.get("group_id")
                            if gid:
                                related_group_ids.add(gid)
        except Exception as search_error:
            logger.warning(f"Graphiti semantic search failed: {search_error}")
        
        # è®¡ç®—æ¯ä¸ªEpisodeçš„ç›¸ä¼¼åº¦
        similar_documents = []
        seen_group_ids = set()
        
        for episode in all_episodes:
            doc_group_id = episode.get("group_id")
            if not doc_group_id or doc_group_id in seen_group_ids:
                continue
            
            # æ’é™¤è‡ªå·±
            if group_id and doc_group_id == group_id:
                continue
                    
            # è®¡ç®—ç›¸ä¼¼åº¦
            content = episode.get("content", "") or ""
            document_name = episode.get("document_name", "") or ""
            
            # æ–¹æ³•1: å¦‚æœgroup_idåœ¨related_group_idsä¸­ï¼Œè¯´æ˜è¯­ä¹‰æœç´¢æ‰¾åˆ°äº†ç›¸å…³ç»“æœ
            similarity_score = 0.0
            if doc_group_id in related_group_ids:
                similarity_score = 0.8  # è¯­ä¹‰æœç´¢åŒ¹é…çš„é»˜è®¤åˆ†æ•°
            
            # æ–¹æ³•2: å…³é”®è¯åŒ¹é…ï¼ˆä½œä¸ºè¡¥å……ï¼‰
            query_lower = query_text.lower()
            content_lower = content.lower()
            name_lower = document_name.lower()
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            matched_chars = 0
            for char in query_lower:
                if char in content_lower or char in name_lower:
                    matched_chars += 1
            
            keyword_score = matched_chars / len(query_text) if query_text else 0.0
            similarity_score = max(similarity_score, keyword_score * 0.6)  # å…³é”®è¯åŒ¹é…æƒé‡è¾ƒä½
            
            # åªä¿ç•™æœ‰ç›¸ä¼¼åº¦çš„æ–‡æ¡£
            if similarity_score > 0.1:
                    seen_group_ids.add(doc_group_id)
                    similar_documents.append({
                        "group_id": doc_group_id,
                    "document_name": document_name,
                    "version": episode.get("version", ""),
                    "created_at": episode.get("created_at"),
                    "similarity_score": round(similarity_score, 3)
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_documents.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
        similar_documents = similar_documents[:limit]
        
        logger.info(f"æ‰¾åˆ° {len(similar_documents)} ä¸ªç›¸ä¼¼æ–‡æ¡£")
        
        return {
            "query": query_text,
            "similar_documents": similar_documents,
            "count": len(similar_documents)
        }
    except Exception as e:
        logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼éœ€æ±‚å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥æ‰¾ç›¸ä¼¼éœ€æ±‚å¤±è´¥: {str(e)}")


@router.post("/qa/analyze")
async def qa_analyze_requirement(
    group_id: str = Query(..., description="æ–‡æ¡£ group_id"),
    provider: str = Query("qianwen", description="LLMæä¾›å•†")
):
    """
    éœ€æ±‚åˆ†æ
    
    åˆ†æéœ€æ±‚æ–‡æ¡£çš„å®Œæ•´æ€§ã€ä¸€è‡´æ€§ï¼Œæä¾›æ”¹è¿›å»ºè®®
    """
    try:
        # æŸ¥è¯¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰ Episode
        episode_query = """
        MATCH (e:Episodic)
        WHERE e.group_id = $group_id
        RETURN e.uuid as uuid, e.name as name, e.content as content,
               e.version as version, e.created_at as created_at
        ORDER BY e.created_at ASC
        """
        episodes = neo4j_client.execute_query(episode_query, {"group_id": group_id})
        
        if not episodes:
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æ–‡æ¡£: group_id={group_id}")
        
        # æ„å»ºæ–‡æ¡£å†…å®¹æ‘˜è¦
        document_summary = []
        for ep in episodes:
            ep_name = ep.get("name", "")
            ep_content = ep.get("content", "")[:500]  # é™åˆ¶é•¿åº¦
            document_summary.append(f"ã€{ep_name}ã€‘\n{ep_content}\n")
        
        full_content = "\n".join(document_summary)
        
        # ä½¿ç”¨ LLM åˆ†ææ–‡æ¡£
        analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹éœ€æ±‚æ–‡æ¡£çš„å®Œæ•´æ€§ã€ä¸€è‡´æ€§å’Œè´¨é‡ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{full_content}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. **å®Œæ•´æ€§**ï¼šæ–‡æ¡£æ˜¯å¦åŒ…å«äº†éœ€æ±‚æ–‡æ¡£åº”æœ‰çš„æ‰€æœ‰éƒ¨åˆ†ï¼ˆæ¦‚è¿°ã€åŠŸèƒ½éœ€æ±‚ã€éåŠŸèƒ½éœ€æ±‚ã€ç”¨ä¾‹ç­‰ï¼‰
2. **ä¸€è‡´æ€§**ï¼šæ–‡æ¡£å†…éƒ¨æ˜¯å¦å­˜åœ¨çŸ›ç›¾æˆ–ä¸ä¸€è‡´çš„åœ°æ–¹
3. **æ¸…æ™°åº¦**ï¼šéœ€æ±‚æè¿°æ˜¯å¦æ¸…æ™°ã€æ˜ç¡®ã€å¯ç†è§£
4. **å¯è¿½æº¯æ€§**ï¼šéœ€æ±‚ä¹‹é—´æ˜¯å¦æœ‰æ¸…æ™°çš„å…³è”å…³ç³»
5. **æ”¹è¿›å»ºè®®**ï¼šé’ˆå¯¹å‘ç°çš„é—®é¢˜ï¼Œæä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®

è¯·ä»¥ç»“æ„åŒ–çš„æ–¹å¼è¾“å‡ºåˆ†æç»“æœã€‚"""
        
        analysis_result = await llm_client.chat(
            provider=provider,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªéœ€æ±‚æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æéœ€æ±‚æ–‡æ¡£çš„è´¨é‡å’Œæ”¹è¿›å»ºè®®ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ],
            temperature=0.3
        )
        
        logger.info(f"éœ€æ±‚åˆ†æå®Œæˆ: group_id={group_id}")
        
        return {
            "group_id": group_id,
            "analysis": analysis_result,
            "episode_count": len(episodes)
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"éœ€æ±‚åˆ†æå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"éœ€æ±‚åˆ†æå¤±è´¥: {str(e)}")

